{
  "values": [
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 1.\nСовременное состояние Computer Science характеризуется тем, что, помимо естественных данных — результатов научных наблюдений, метеорологических данных, социологических и др., — появляется огромное количество данных, связанных\nсработой информационных систем. Эти новые данные существенно отличаются от тех, что анализировались на заре компьютерной эры. Те старые данные (их можно условно назвать естественнонаучными) в основном требовали математической обработки.\nВ отличие от них данные современных информационных систем (большие данные) не могут быть представлены простыми математическими моделями, чьи параметры следует определить. Кроме того, эти данные отличаются существенной неоднородностью, разнообразной и непредсказуемой структурой, и зачастую непонятно, как их обрабатывать и нужно ли это вообще? Можно ли в них найти чтолибо полезное? Этих данных настолько много, что их анализ за разумное время требует вычислительных ресурсов, существенно превышающих вычислительныe ресурсы самой информационной системы. Это значит, что данные часто лежат мертвым грузом, несмотря на скрытые в них закономерности, составляющие полезную информацию, которую требуется найти. Поиск таких закономерностей называется Data Mining — добывание данных из груды пустых данных (по аналогии\nспустой породой). Что значит «обрабатывать» данные, и как их добывать? Для ответа на все приведенные вопросы необходимо сначала выяснить, откуда берутся эти большие данные. Их источниками могут быть:\n• социальные сети — посты, комментарии, сообщения между пользователями и пр.;\n• события, связанные с действиями пользователей в веб или мобильных приложениях;\n• логи приложений;\n• телеметрия сети устройств из мира «Интернета вещей» (Internet of Things, IoT); потоки событий крупных вебприложений;\n• потоки транзакций банковских платежей с метаданными (время, место платежа и т.д.).\nВсе эти данные должны быть обработаны в режиме реального времени или же постфактум. В обоих случаях они могут размещаться в различных хранилищах (как общего назначения, так и специализированных) и в разных форматах: CVS, XML, JSON, таблицы в реляционных БД, базах данных NoSQL и пр.\nДля пакетной обработки исторических данных различных форматов, расположенных в разных хранилищах, необходим единый подход,\n\nобеспечивающий выполнение запросов к данным, хранящимся в указанных выше форматах. В настоящее время распространены следующие подходы.\n1.Преобразование данных из различных форматов в общий, допускающий выполнение запросов к единообразным данным. Это можно сделать с помощью облачных сервисов трансформации и копирования, таких как Azure Data Factory и AWS Glue, которые консолидируют данные из разных источников в один. Такое хранилище традиционно называется Data Warehouse (DWH, «склад данных»), а преобразование данных — ETL (Extract Transform Load — «извлечение, преобразование, загрузка»). Данный подход достаточно распространен в традиционных системах, в которых DWH строится на основе кластера SQLсерверов. Подход позволяет использовать все элементы синтаксиса SQL.\n2. Складирование данных в единое хранилище без изменения формата. При этом форматы данных остаются прежними (JSON, XML, CSV и т. д). Такое хранилище, в котором данные размещаются в виде несвязанного набора данных, называется Data Lake («озеро данных»). Файловая система, лежащая в основе подобных хранилищ, совместима с HDFS — распределенной файловой системой, которая, в свою очередь, совместима с Hadoop (Azure Data Lake, AWS EMRFS). Такое хранилище позволяет задействовать сервисы из экосистемы Hadoop (например, Hive, Apache Spark и др.) и применять иной подход к операциям\nс данными: ELT (Extract Load Transform — «извлечение, загрузка, преобразование»), когда данные можно трансформировать после загрузки. При этом используется сервер аналитики или кластер серверов, содержащий процессор специализированного языка запросов, в котором все разнородные источники данных представляются как внешние источники данных, к которым применим SQLподобный синтаксис. Для подобного хранилища также может использоваться подход MapReduce (будет более подробно описан далее) и обработка данных в оперативной памяти (in memory processing).\n3. Кроме того, для обработки потоковых данных существуют специализированные сервисы, допускающие обработку потока сообщений с помощью SQLподобного синтаксиса (например, сервисы Azure Stream Analytics, AWS Kinesis Analytics) или программных структур (Apache Spark Streaming), а также сервисы для приема и концентрации этих сообщений (например, Azure Event Hub, Kafka, Azure Spark и пр.).\nПрежде всего большие данные - это огромные массивы данных или потоки,\nкоторые содержат подлежащую извлечению информацию, или же умеренно большие объемы данных, требующие быстрой интерактивной обработки с целью исследования, проверки гипотез, тренировки алгоритмов машинного обучения. Для выполнения этой обработки необходим высокий уровень параллелизма,\n\nбольшой объем оперативной памяти (для in memory processing), что достигается применением кластеров виртуальных машин. Вот тутто и проявляются все преимущества облачных сред: модель IaaS позволяет создавать кластеры удалять их по требованию с минимальными затратами. Виртуальные серверы создаются и задействуются только в течение того промежутка времени, когда они нужны, и, соответственно, плата за них взимается только во время прямого использования. Но просто создание кластера для обработки больших данных с последующей установкой требуемых программ и их настройкой — весьма трудоемкое занятие. Кроме того, облачные провайдеры предоставляют отдельные сервисы PaaS больших данных.\nОбработка больших данных\nБольшие данные могут быть обработаны в пакетном режиме, когда они уже присутствуют в хранилище. Чаще всего это необходимо для агрегирования данных и построения аналитических отчетов на их основе. Рассмотрим в качестве примера систему мониторинга электроэнергии сети зданий. В этой системе замеры потребляемой мощности передаются с малой периодичностью как сообщения от каждого измерительного модуля. Чтобы получить величину дневного потребления электроэнергии, необходимо сложить все замеры сизмерительного модуля каждого пользователя в отдельности. Если итоговый результат нужно, к примеру, формировать в виде ежедневного (еженедельного, ежемесячного и пр.) отчета, то наиболее просто реализовать такую систему следующим образом.\nАрхитектура системы учета электроэнергии, построенная на основе разделения хранилищ сырых и агрегированных данных — так называемая лямбдаархитектура\nВсе сообщения от измерительных устройств можно хранить в нереляционном хранилище табличного типа (этот вопрос подробнее рассматривается в части II), например HBase или Cassandra. Каждая строка таблицы будет содержать\n \nвременну'ю метку (то есть время поступления или отправления сообщения), идентификатор устройства, его отправившего, и собственно величину замера.\nДля построения периодического отчета с помощью системы бизнесаналитики (business intelligence, BI) (например, PowerBI или Microsoft SSRS) или отображения этой величины в браузере необходимо, чтобы данные в агрегированном виде были размещены в БД SQL. А почему нельзя сразу размещать их непосредственно\nвэтой базе? Посчитаем. Предположим, что измерительное устройство отсылает сообщения каждые пять минут. Это значит — 12 отсчетов в час, или около 105 тыс.\nвгод. Теперь предположим, что система мониторинга собирает данные энергопотребления с каждого устройства заказчика, которых могут быть десятки, а самих заказчиков — тысячи. В итоге таблица, хранящая события в реляционной БД, будет содержать многие миллионы строк: допустим, при наличии 100 заказчиков со средним числом подключенных приборов 20 за год такая таблица пополнится 200 миллионами строк. Вот они, большие данные!\nПростое применение запроса на выборку данных к подобной таблице может занять очень много время. А если добавить еще необходимость постоянных запросов на обновление таблицы поступающими от устройств сообщениями, а также постоянные запросы от вебпортала или от пользователей на получение отчетов, то станут очевидны будущие проблемы такой архитектуры с одной базой данных. Пакетная же обработка с группировкой и суммированием результатов может быть выполнена, например, с использованием Hadoop MapReduce или Apache Spark. Сами алгоритмы группировки в данном случае можно реализовать с помощью программ на Java (для MapReduce, Spark) или Python (Spark) и запустить в кластере.\nВитоге размеры таблицы с агрегированными данными в базе данных SQL будут существенно меньше, чем в таблице с сырыми данными. Так, если хранится часовое агрегирование, то в SQLтаблице в 12 раз меньше строк, чем в NoSQL, а если суточное — то в 288 раз. При этом для клиентов запрос на получение данных будет очень простой и высокопроизводительный: выбрать из таблицы агрегатов строки, отфильтрованные по идентификатору заказчика и по требуемому временно'му интервалу без каких бы то ни было группировок в самом запросе.\nПо поводу подобной архитектуры следует сделать целый ряд замечаний.\n1.Обеспечить высокую точность позволяет большое количество замеров, следующих с малым временны'м интервалом. Если интервал постоянный, то можно упустить включение/выключение прибора, произошедшее между замерами. Эта проблема решается путем передачи не периодических замеров, а замеров, приуроченных к событию: включению или изменению величины проходящей мощности более чем на заданную величину. Такое решение, вопервых, разгрузит сеть от слишком частых передач отсчетов (но не\n\nполностью — необходимо оставить периодические сообщения от измерителя о его работоспособности), и вовторых, существенно уменьшит объем сырой таблицы.\n2. В случае проблем с сетью и для обеспечения высокой точности вместе с требованием разгрузки сети необходимо физическое разделение отсчетов замера мощности на уровне АЦП измерительного прибора (они могут быть выполнены с высокой частотой дискретизации) и отсылка результатов суммирования измерений в виде сообщения в систему. Чтобы обеспечить это разделение, измерительное устройство должно быть достаточно «интеллектуальным»: обладать памятью и иметь возможность синхронизировать часы, чтобы установить временну'ю метку. Может показаться, что достаточно иметь момент времени приема сообщения, но это неверно. Для получения высокой точности и обеспечения надежности важно каждое сообщение. Они могут быть потеряны как изза отказа измерительного устройства, так и изза проблем с телекоммуникационной сетью. Чтобы устранить проблемы с сетью, устройство может запоминать сообщения и пересылать их, когда сеть восстановит работоспособность. И вот тутто очень важно, чтобы каждому сообщению была присвоена метка времени: когда оно сгенерировано. Это позволит в итоге правильно подсчитать суммарное энергопотребление в заданный временной интервал.\nТеперь рассмотрим еще один вопрос: а зачем вообще нужно промежуточное хранилище такого большого объема?\nАрхитектура системы учета электроэнергии, построенная на основе единого реляционного хранилища данных\nВедь можно периодически очищать сырую таблицу в SQLхранилище после заполнения данными агрегированной таблицы и хранить только результаты агрегирования. Действительно, при наличии данных энергопотребления в\n \nтечение каждого дня недели/месяца/года можно легко подсчитать суммарное энергопотребление за бо'льшие периоды времени. Против такого подхода есть ряд возражений. Например, возможна ситуация, когда изза проблем с сетью не все устройства отослали свои данные вовремя. И если сырая таблица очистится перед тем, как восстановится работоспособность сети и устройства сбросят свои данные, то последние останутся неучтенными и не будет никакого смысла дополнительно усложнять измерители в виде внутреннего буфера сообщений и синхронизиру емых часов. В то же время реализация дополнительной логики, обеспечивающей пересчет агрегированных данных в пакетном режиме при приеме недостающих сообщений, позволит произвести правильный и надежный подсчет потребления даже при ненадежной сети передачи данных.\nТут мы сталкиваемся еще с одним аспектом анализа больших данных: потоковой обработкой. В данном случае необходимо из всего потока сообщений обнаруживать те, чья временна'я метка меньше, чем текущее время минус самая большая временная задержка, и при их обнаружении запускать внепланово или планировать дополнительно запустить в определенное время (если построение агрегированных таблиц происходит в конкретное время, скажем по ночам) задачу по повторному пересчету. Для этой цели может служить, например, Apache Storm, Apache Spark Streaming или же Apache Pig.\nСледующая причина, побуждающая оставитьтаки сырую таблицу без удаления данных, — возможность провести интерактивный анализ хранящихся в ней данных. То есть применить к ним запросы на специальном языке, позволяющем комбинировать, фильтровать, группировать, проводить арифметические операции в режиме реального времени. Это позволяет находить скрытые закономерности в данных, например определять пики энергопотребления, устанавливать корреляцию их с внешними событиями (скажем, с погодой), определять профили пользователей, характеризующиеся оптимальным энергопотреблением. Или для одного пользователя строить типовой профиль применения энергоресурсов и обнаруживать отклонения от него (допустим, утечку электроэнергии, несвоевременное выключение освещения и пр.). Подобные задачи могут решаться с помощью системы интерактивного анализа данных, таких как Spark SQL или Apache Hive, арасширенный интеллектуальный анализ — благодаря применению библиотеки машинного обучения Spark MLib.\nАрхитектура, содержащая в своем составе хранилища как сырых данных, так и агрегированных, является очень гибкой и позволит создать не просто очередную систему учета электроэнергии, но и интеллектуальную, которая может «подсказать», как уменьшить потребление энергии, где есть узкие места, а это уже принципиально новый уровень по сравнению с простой телеметрией. Такое развитие возможно благодаря тому, что данные в ней хранятся в том виде, в котором они наиболее удобны для выполнения анализа различными сервисами:\n\nдля обычного сервиса построения отчетов об энергопотреблении — в агрегированном виде, а для целей Data Mining — в сыром.\nКлючевой момент всех технологий, работающих с большими данными, — возможность распараллеливания выполняемых задач, областей хранения, памяти и т. д. между группой компьютеров (кластер). Кроме того, эти технологии должны:\n• обладать возможностью линейного масштабирования и наращивания производительности путем добавления новых серверов в кластер. Линейность масштабирования означает пропорциональность производительности/объема хранения количеству компьютеров в кластере;\n• иметь специальную файловую систему для надежного хранения и доступа\n• данным, позволяющую оперировать очень большими объемами данных и допускать их репликацию в целях повышения надежности и производительности;\n• позволять выполнять запросы к файлам с помощью специального языка запросов или программного интерфейса;\n• иметь планировщик, позволяющий распределять эти запросы среди узлов кластера для обеспечения их параллельной работы.\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 2.\nОблачные провайдеры позволяют подключать различные устройства, программные продукты, сервисы (игровые устройства, стационарные устройства IoT, подключенные автомобили, мобильные приложения, вебсерверы и серверы приложений, медиасервисы и пр.) через специальные сервисы концентраторов сообщений и шлюзы устройств «Интернета вещей» (IoT Gateway). Концентраторы (AWS Kinesis Stream, Azure Event Hub) обеспечивают однонаправленный прием сообщений извне в облако, а IoTшлюз (Azure IoT Hub) — двунаправленную коммуникацию с устройствами, то есть возможность обратной отсылки команд устройствам из облака. Этот поток может быть обработан сервисами потоковой обработки и анализа.\nК сервисам потокового анализа относятся сервисы, которые допускают интерактивный анализ потока данных и позволяют создавать аналитические запросы на специальном языке (чаще всего с SQLподобным синтаксисом), интерактивно их применять и отображать результаты (например, Azure Stream Analytics). К сервисам потоковой обработки относятся те, которые задействуют модули, написанные на компилируемых языках для построения потоковых задач анализа (таких как Apache Storm в HDInsight или AWS EMR) и не допускающие интерактивного использования.\nОблачные сервисы, относящиеся к большим данным\n \nСообщения, полученные от концентраторов или IoTшлюзов, могут быть направлены по своим назначениям, в зависимости от внутренних признаков (такая возможность обеспечивается системой маршрутизации сообщений: Azure Event Grid или аналогичной системой в IoTшлюзе), или целиком направлены в облачное хранилище. Это может быть либо хранилище общего назначения (типа Azure BLOB Storage или AWS S3), либо HDFSсовместимое (Azure Data Lake). Кроме того, данные в такое хранилище могут доставляться сервисами копирования и трансформации данных (AWS Glue или Azure DataFactoy) или специа лизированными сторонними программами через предоставляемые этими сервисами API. Источниками данных могут быть внешние реляционные и нереляционные базы данных и файлы. После размещения в облачном хранилище информацию можно обработать в пакетном режиме (например, с помощью Hadoop MapRerduce в Azure HDInsight или AWS EMR), интерактивном режиме (Azure Data Lake Analytics, AWS Athena) или с применением машинного обучения. Результаты обработки могут быть размещены в реляционном хранилище данных и доступны для средств BI (Microsoft PowerBI или AWS QuickSight).\nЛюбой облачный провайдер предоставляет сервисы IaaS, позволяющие создать ряд виртуальных машин, которые можно объединить в кластер. В этом случае пользователю придется самому создавать и конфигурировать нужный BigDataфреймворк. Ситуация в какойто мере облегчается тем, что имеются готовые образы виртуальных машин с предустановленными утилитами и компонентами. Но реализация IaaSрешения требует достаточно высокой квалификации у пользователей облачных сред. Ведь, помимо навыков инсталляции и конфигурирования образцов виртуальных машин, необходимо оперировать сервисами IaaS, чтобы построить нужную инфраструктуру, что требует больших затрат времени и обширных знаний, не относящихся к области BigData. Частично задачу упрощает тот факт, что облачные сервисы можно создавать с помощью шаблонов: AWS CloudFormation или Azure ARM Template. Но остаются сложности интеграции IaaSрешения с другими сервисами, сервисами логирования и мониторинга.\nВ то же время для каждого описанного компонента общей архитектуры, у облачных провайдеров Microsoft Azure и AWS существует свой PaaSсервис. Но у обоих провайдеров для ряда компонентов имеются два вида сервисов.\nК первому виду относятся сервисы, которые целиком применяют существующие фреймворки больших данных из экосистемы Hadoop. Задействуются стандартные средства взаимодействия с пользователем (например, Jupiter Notebook для написания запросов и отображения результатов). При этом создание, конфигурирование и масштабирование кластера полностью автоматизировано. К таким сервисам относятся Azure\n\n HDInsight и AWS EMR. Они позволяют разворачивать кластеры Hadoop, Spark, Storm, HBase, Kafka и RServer (только Azure). Кроме того, компоненты облачных сервисов могут быть размещены на кластерах AWS ECS / EKS или Azure Kontainer Services в Docker контейнерах. Ко второму виду относятся облачные сервисы, целиком и полностью (нативно) предоставляемые облачными провайдерами. Такие сервисы конфигурируются и управляются только из облачного портала или через его REST API и не используют сторонние средства и сервисы.\nАрхитектуры традиционных информационных систем\nДля получения представления о том, что такое большие данные, как они возникают и обрабатываются, следует подробнее разобраться с тем, что собой представляют современные информационные системы. Системы обработки больших данных подразумевают возможность, позволяющую реализовать не только систему обработки данных, но и приложение, которое решает конкретную бизнес-задачу. Эта идея — асинхронная передача сообщений (событий) между компонентами системы и применение отдельных сервисов, обеспечивающих надежную передачу сообщений. Казалось бы, практически любая информационная система имеет дело с обменом сообщений, представленном в том или ином виде, так что тут нового? Разберемся по порядку.\nМногие годы информационные системы строились в виде одного крупного монолита с единообразной кодовой базой, которая разворачивается на сервере как единое целое: вместе со всеми подключаемыми модулями, библиотеками и т. д. Такой монолит (рис. 3.2) отвечал за все (или почти): за взаимодействие с базами данных, обеспечение системы контроля учетных данных, работу периодических сервисов синхронизации, логирования и пр.\nНиже представлены проблемы, свойственные такой архитектуре.\n• Любое изменение в любом компоненте монолита, даже самое незначительное, требует компилирования и разворачивания всей системы, что при большом размере монолита задача весьма небыстрая.\n• Монолит очень плохо масштабируется и подвержен существенным проблемам использованием ресурсов. Действительно, как правило, подобные архитектуры разворачиваются на одном сервере или в виде полных копий на группе серверов. Поэтому любой компонент (или компоненты), который задействует ресурсы сервера (скажем, оперативную память, процессор и т. д.), в значительной степени может затруднить работу всего монолита и потребует увеличения производительности всего сервера, на котором расположена данная\n\nархитектура. Компоненты монолита невозможно масштабировать\nнезависимо.\n• Монолит жестко диктует стек технологий программирования, и\nобновить его, также обновить саму архитектуру — значит, по сути, переписать весь код заново. Сюда же можно отнести проблемы с быстрым «старением» монолита, склонностью к обрастанию спагеттикодом, а также «тупиковой внутренней архитектурой» — это когда невозможно исправить баг или улучшить чтолибо, не вызвав появления нового бага или ухудшения системы.\n• Сопровождение монолита может причинить множество неудобств программистам, системным администраторам, тестировщикам...\nМонолитная архитектура\nРешить указанные проблемы была призвана многослойная архитектура\n  \nМногослойная архитектура\nВ ней за специфическую задачу отвечает отдельный слой: пользовательского интерфейса (user interface layer, UI layer), бизнеслогики (business logic layer, BL layer) и доступа к данным (data access layer, DAL).\nРассмотрим эту архитектуру подробнее. Каждый слой отвечает только за определенную группу задач. UIслой содержит только вебсерверы, являющиеся источником вебстраниц, или размещает сервисы (REST, SOAP или др.) для взаимодействия с клиентскими приложениями. Кроме того, данный слой принимает запросы от клиентов и транслирует их слою бизнеслогики. Поскольку с клиентом напрямую взаимодействует только UIслой, то на одном уровне с ним в тесной интеграции находится сервис аутентификации клиентов. Слой BL содержит серверы приложений и отвечает, собственно, за бизнеслогику и интеграцию со сторонними сервисами. Слой DAL обеспечивает программный доступ слоя BL к базе данных и файловому хранилищу.\nПодобная архитектура по сравнению с монолитной имеет следующие преимущества.\n• Разделение на слои позволяет реализовать в каждом слое наиболее подходящий для него стек технологий. Можно независимо обновлять фреймворки на каждом слое.\n• Логическое разделение на слои существенно упрощает процесс разработки и сопровождение всей системы. Распределение команд программистов по слоям и специализация разработчиков (фронтенд, бэкенд), уменьшение объема кода на каждом слое, а также независимый деплой (развертывание) значительно улучшают качество всей системы, делая ее более гибкой и пригодной для сопровождения.\n• Возможно независимое масштабирование каждого слоя.\nНаиболее часто подобная архитектура реализуется в виде серверов, расположенных в локальной сети, разбитой на подсети с настроенными фаерволами. Адреса серверов (IP или URL) разных слоев и учетные данные для доступа к ним прописаны в конфигурационных файлах других серверов. В этой архитектуре уже необходимо централизованно хранить учетные данные, иметь серверы DNS, сервис хранения логов и мониторинга, а также сервер для администрирования.\nВмонолитной архитектуре все это могло быть расположено на одном большом общем сервере. Логирование также было весьма простым, поскольку вся система строилась в рамках одного стека технологий. В многослойной архитектуре каждый слой может генерировать логи,\n\nсущественно отличающиеся от логов другого слоя. Кроме того, увеличение количества серверов тоже ведет к увеличению количества и видов логов.\nМногослойная архитектура информационных систем в настоящее время чрезвычайно распространена, она более гибкая и удобная по сравнению с монолитной, однако не может решить ряд проблем. В частности, при разрастании проекта до такого масштаба, что слой BL становится сопоставимым с монолитом, появляются аналогичные проблемы с масштабированием, производительностью, сопровождением и т. д.\nСледующая разновидность многослойной архитектуры — SOA: сервис- ориентированная архитектура. Ее квинтэссенцией является архитектура микросервисов, суть которой заключается в том, что каждое приложение разбивается на наименьшие самостоятельные модули, и каждый из них отвечает только за один аспект: отсылку писем, загрузку и выгрузку файлов и пр. Каждый такой сервис можно совершенно независимо развернуть и обновить в любой момент, не затрагивая остальные сервисы. Код подобного сервиса может быть совсем небольшим, и для его сопровождения нужна маленькая команда. Более того, все микросервисы можно писать на разных языках, лучше всего подходящих для решения текущей задачи. И наконец, каждый микросервис может быть развернут на своем сервере или в кластере серверов, которые могут быть совершенно независимо масштабируемы. Каждый сервис доступен по URL конечной точки и из них, как из элементов конструктора, можно собрать новые системы, а каждый сервис использовать в различных системах.\nАрхитектура микросервисов\n \nОднако работа с микросервисной архитектурой предполагает ряд трудностей.\n• Большое количество сервисов, размещенных на многих серверах, означает большое количество разнообразных логов. По сути, вот они, большие данные! Анализ этих данных, определение метрик производительности, узких мест, поиск логов и отображение результатов в виде графиков в режиме, близкому режиму реального времени, требует не просто сервиса, но целой подсистемы, сопоставимой с основной системой. В качестве примера такой подсистемы можно привести ELK стек — Elasticsearch (хранение логов и поиск), Logstash (агенты, обеспечивающие доставку логов с серверов в кластер Elasticsearch) и Kibana (интерактивные диаграммы, графики и др.), Splunk.\n• Взаимодействие между сервисами происходит преимущественно с помощью REST. А потому каждый сервис должен знать URL всех сервисов, с которыми он может потенциально взаимодействовать.\n• Если по какойто причине запрос не был обработан (например, сервис был недоступен), то для его повторения необходимо организовать логику повтора в рамках самого кода.\nСинхронизировать работу микросервисов позволит отдельный сервис, обеспечивающий надежную доставку сообщений, а также предоставляющий малое количество конечных точек. И вот тут мы постепенно подходим к Event Driven Design — архитектуре, основанной на обмене сообщениями.\nАрхитектуры, построенные на базе микросервисов, каждый из которых можно разместить в кластере серверов, дополненные сервисами обмена сообщениями (брокерами сообщений — message brokers), могут быть чрезвычайно масштабируемыми. Для этого нужно, чтобы сами сервисы обмена сообщениями были масштабируемыми. Брокеры, как правило, имеют архитектуру с одним или несколькими головными узлами, отвечающими за управление кластером, и исполнительными узлами, на которых выполняются необходимые вычисления и хранятся данные (сообщения). Чтобы обеспечить надежность кластера, данные могут быть реплицированы между исполнительными узлами.\n \nОбщая структура масштабируемой системы, построенной на основе кластера\nПодобные структуры весьма сложны: кластеры серверов, балансировщики нагрузки, системы управления конфигурацией, логирования и пр.\nИсейчас самое время напомнить преимущества облачных платформ: все подобные системы представляются как сервисы — AWS ECS и Azure Service Fabric. Данные сервисы отвечают за масштабирование, мониторинг, развертывание приложений или контейнеров на них.\nВ облачных средах, наряду с сервисами PaaS, отвечающими за обработку больших данных, Azure и AWS предоставляют сервисы, занимающие промежуточное положение между IaaS и PaaS. Эти сервисы позволяют применять наиболее популярные фреймворки, работающие с большими данными, — Apache Spark, Storm, Kafka, HBase, Storm и ряд других. К таким сервисам относятся AWS EMR и Azure HDInsight. Они берут на себя все трудности, связанные с настройкой и конфигурированием сервисов в кластерах, а конечному пользователю предоставляют готовый и настроенный сервис. Это очень большое преимущество подобных сервисов по сравнению с ручным конфигурированием кластера из виртуальных машин. Практически все сервисы Apache Hadoop являются продуктами с открытым исходным кодом, имеют подробную документацию и поддержку обширного сообщества, однако их установка, настройка и конфигурирование на практике очень трудны.\nСервисы AWS EMR и Azure HDInsight занимают промежуточное положение между IaaS и PaaS, сочетая все возможности сервисов Apache с простотой PaaS. Эти сервисы представляют собой надстройки над набором виртуальных машин. Последние создаются, запускаются, останавливаются и удаляются только на время выполнения задания в сервисе или по мере надобности (как в случае Apache Kafka, например). Сервисы AWS EMR и Azure HDInsight очень хорошо интегрируются сервисами копирования и трансформации данных (Azure Data Factory), составляя, по сути, вычислительные ресурсы для заданий ETL. При создании кластеров с портала для HDInsight и AWS EMR указываются размеры виртуальных машин и сценарий, который должен выполняться при запуске сервиса. Создание и удаление управляемых кластеров полностью автоматизируемо, что и обусловливает удобство их использования в случае построения систем пакетного анализа, копирования и трансформации данных.\nБессерверные архитектуры\nАрхитектура, основанная на обмене сообщениями, характеризуется тем, что все компоненты системы взаимодействуют через малые порции сообщений, требующие вычислительных ресурсов только непосредственно в момент обработки сообщений. Все остальное время сервисы заняты только\n\nтем, что прослушивают конечные точки сервисов обмена сообщениями. Во время прослушивания серверы или виртуальные машины не загружены. Но в облачных средах принимается плата за включенную виртуальную машину вне зависимости от того, насколько интенсивно она используется. Таким образом, если поток сообщений не слишком сильный, то сервисы работают вхолостую. Но зачастую необходимо выполнить программу однократно или в непредсказуемые моменты времени в ответ на поступившее сообщение, для чего использовать виртуальную машину нецелесообразно. Для такого случая облачные провайдеры обеспечивают возможность выполнения кода по требованию без предоставления серверов — бессерверные сервисы (serverless). Подобный сервис от Microsoft называется Microsoft Azure Function, а от AWS — AWS Lambda. Последний является классическим вариантом. Этот сервис позволяет размещать исполняемый код и запускать его внешним событием, которое может прийти от сервисов SNS, SQS, Kinesis Stream, S3, API Gateway и др. Сам сервис имеет различные уровни производительности процессора и памяти, которые используются лишь при исполнении кода, и плата начисляется только этот момент.\nКонцепция serverless очень удобна для построения приложений с малым, средним и умеренно большим потоком сообщений.\nГлавные преимущества serverless таковы:\n• более дешевая среда исполнения кода, чем виртуальные машины, при небольших, средних и умеренно больших потоках;\n• более высокая надежность сервиса, чем в случае одиночной виртуальной машины, — SLA на уровне 99,9 %;\n• простота интеграции с другими облачными сервисами, не требующая внесения изменения в код;\n• простота развертывания кода и конфигурирования — необходимо только добавить код напрямую или через ZIP файл и выбрать уровень производительности.\nОднако подобным сервисам присущи и недостатки:\n• ограниченная масштабируемость;\n• ограниченное время выполнения кода;\nНо вместе с тем serverless сервисы крайне удобны для построения архитектур, основанных на событиях (event driven design).\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 3.\nОблачные хранилища файлов — это сервисы, которые обеспечивают хранение файлов, предоставляют доступ к ним с помощью REST API, позволяют скачивание по прямой ссылке (постоянной или с конечным сроком действия) и в некоторых случаях предоставляют доступ к файловой системе по протоколу SMB.\nХранить файлы, содержащие большие данные, в облачном хранилище очень дешево и в ряде случаев удобнее, чем в других хранилищах. Наиболее типовой пример размещения данных — это хранение файлов логов приложения, которые копируются туда периодически из сервера источника или создаются и заполняются специальной программой клиентом, размещенной на сервере источнике логов.\nКроме того, в облачных файловых хранилищах могут размещаться виртуальные жесткие диски (virtual hard drive, VHD) облачных виртуальных машин, на которых,\nвсвою очередь, тоже можно размещать файлы. Но в этом случае ответственность за доступность информации ложится на владельца виртуальных машин. Рассмотрим подробнее различные форматы хранения больших данных.\n Структура потока логов серверов в облачное хранилище\n\nФорматы хранения данных\nКак уже отмечалось, наиболее типовой случай использования текстовых файлов для хранения больших данных — хранение логов приложений в том или ином текстовом формате. Такие файлы могут иметь расширения .log, .txt, .csv и др. Общее у этих форматов то, что логи в них, по сути, хранятся в виде таблицы (отсюда и название — табличный формат), каждая строка которой — запись конкретного события. Строка состоит из набора столбцов, разделенных некими символами. Это могут быть пробелы, символы табуляции, двоеточие, точка с запятой и т. д. Подобные файлы можно открыть с помощью любого текстового редактора (если они не очень велики), и для их анализа (например, поиска запросов с 500 ошибками) подойдут стандартные утилиты командной оболочки (grep, awk и пр.) или же специализированные сервисы аналитики (к этому вопросу мы еще вернемся). Кроме того, табличная структура подобного файла очень удобна для импорта в реляционную или нереляционную СУБД табличного типа. Особенность таких файлов — линейная структура со строго одинаковым количеством столбцов в каждой записи и в общем случае линейное время доступа к записям. Строго говоря, возможны ситуации, когда разные строки могут содержать различное количество столбцов. Например, запись в лог-файле, отражающая ошибку приложения и трассировку стека, может включать гораздо меньше строк, чем запись, отражающая какое-либо событие, характеризующее нормальную работу приложения. В общем же случае записи логов содержат как минимум временную метку. Остальные поля (уникальный идентификатор, URL, сообщение ошибки и пр.) могут и отсутствовать. Возникает вопрос: полезна ли запись с одним полем (временной меткой)? Да. Скажем, необходимо проанализировать посещаемость сайта во времени. Просуммировать запросы для заданного временного интервала (например, 15 минут) поможет именно временная метка.\nТабличные файлы имеют и ряд неудобств в использовании. Во-первых, для доступа к элементу информации необходимо знать номер строки и номер столбца. Нет универсального стандарта или языка запросов (за исключением SQL-подобного языка специализированных сервисов аналитики, но об этом позже), что весьма затрудняет анализ логов. Во-вторых, в таких форматах очень просто добавить новую запись в конец файла, но крайне трудно и затратно вставить, удалить или изменить произвольную строку. Как правило, все программные библиотеки вводавывода позволяют добавлять строку в конец файла с помощью стандартных средств, но для произвольной манипуляции данными программисту нужно будет создать отдельную логику в коде, и эта логика весьма непроста.\n\nПомимо текстового файла с табличной структурой, широко распространено хранение информации в более структурированных форматах JSON, XML. Их преимущества в том, что они очень удобны для сериализации/десериализации информации в объектноориентированном виде в коде программы. Кроме того, для обоих форматов существуют стандарты выполнения запросов на выборку данных (для XML — XPath, XQuery, для JSON — JSONPath, JSONQuery). Кратко рассмотрим эти форматы.\nJSON представляет собой формат, при котором данные хранятся в текстовом виде как объект JavaScript (аббревиатура образована от JavaScript Object Notation). В основе любого документа JSON лежит объект, который состоит из набора «ключ — значение». В качестве ключа выступают текстовые величины. В качестве значений допускаются следующие типы.\n• Атомарный тип (для ключей PlayerId, StepId) — величина, состоящая из одного конкретного значения: строки, числа, временно'й метки.\n• Массив (например Players) — группы величин одного типа. (Это не совсем точное определение, в общем случае все элементы массива могут быть совершенно разного типа, но, как правило, коллекции объектов, сериализуемые в JSON в реальных программах, будут иметь одинаковый тип.) В качестве типов элементов массива могут выступать атомарные типы, другие массивы или другие объекты. Массивы обозначаются прямоугольными скобками — [ ], а элементы в массиве разделяются запятыми, допустим: [ \"a\", \"c\", \"d\" ]. Доступ к элементу происходит по числовому индексу, например [0].\n• Объект — коллекция «ключ — значение». Обозначается фигурными скобками { } и имеет синтаксис {\"key\": value}, где value может иметь атомарный тип, тип массива и объекта. По сути, эта коллекция является ассоциативным массивом, в котором для доступа к значению необходимо использовать строковый ключ.\nСтоит заметить, что документ JSON может содержать в качестве корневого элемента не объект, а массив, например: [ { \"key\": 1 }, { \"key\": 2 }, { \"key\": 3 } ]. Это допустимо. Но в примерах, приводимых в книге, вы будете встречаться с JSONдокументами с корневыми элементами типа «объект».\nПреимущество этого формата текстового файла — возможность описания структур произвольной глубины вложенности (объект внутри объекта, коллекция внутри объекта и пр.), что невозможно в случае табличного представления. Кроме того, существенно упрощается сериализация и десериализация объектов, особенно из JavaScriptприложения. Обработка\n\nJSONфайлов в специализированных СУБД (DocumentDB) описана в книге ниже, а некоторые реляционные базы данных (например, PostgresSQL) широко поддерживают этот формат. Очень важным преимуществом формата JSON является то, что он позволяет строить информационные системы с помощью одной и той же технологии на всех уровнях: начиная с документоориентированной БД, хранящей данные в формате JSON (Azure DocumentDB, MongoDB), бэкенда на основе диалекта JavaScript (например, Node.js) и заканчивая фронтендом на основе того или иного фреймворка JavaScript (например, ReactJS, Angular). Это уникальная возможность, реализованная сейчас только в рамках JavaScript/JSON (например, популярен фреймворк MEAN).\nКнедостаткам JSON можно отнести его «многосимвольность»: для группировки данных используются символы (запятые, скобки, кавычки), которые сами по себе не несут информации. Кроме того, присутствуют имена полей. Перечисленные недостатки проявляются наиболее ярко в случае файлов крупного размера, относящихся к большим данным. Но это неизбежная плата за возможность сериализации/десериализации сложных структур. Избавиться от части избыточных элементов синтаксиса поможет формат YAML, но пока что он используется преимущественно в качестве шаблона конфигурационных файлов (в системах Ansible, CloudFormation и др.), а не для хранения данных. Кроме того, сериализация и десериализация YAML не так широко поддерживается программными библиотеками вводавывода. Выборка значений из документа JSON происходит с первоначальной десериализацией его в объект в программном коде, что наиболее удобно и естественно происходит в языке на основе JavaScript.\nСледующий популярный формат хранения данных в текстовых файлах — XML (eXtensible Markup Language — расширяемый язык разметки). Традиционно он использовался для разметки (markup), то есть структурирования текстовых документов. Термин «язык» (language) говорит о том, что XML содержит строгий набор синтаксических правил, на основании которых можно построить конкретное расширение (extension) этого языка. Рассмотрим, что это значит.\nВ общем случае у XML есть два основных компонента: теги и атрибуты. Тег — синтаксический элемент, ограничивающий конкретную порцию информации: <ИмяТега>Информация в текстовом виде <ИмяТега/>. Любой тег начинается открывающим (<) и закрывающим (>) символами. Информация, представленная в текстовом виде, расположена между тегами, последний из которых состоит из двух символов (/>). Возможны и вложенные структуры тегов и коллекции последних. Атрибуты относятся к конкретному тегу и представляют собой коллекцию «ключ — значение».\nСогласно стандарту XML для тега с конкретным именем существует строго определенный набор атрибутов, причем каждый тег должен содержать\n\nконкретный набор атрибутов или не включать их вовсе. Возможно также построение.\nИтак, язык XML состоит из общих правил построения синтаксиса, а расширение данного языка представляет собой конкретный набор вложенных тегов\nи соответствующих им атрибутов, который обеспечивает упорядоченное представление конкретной структурированной информации. Существует стандартизированный способ преобразования информации из одного XML в другой с помощью XSLT — eXtensible Stylesheet Language Transformation. Это тоже XML, элементами которого является не информация, а правила, по которым она из одного типа XML преобразуется в другой. Кроме того, есть XSD (XML Schema Definition) — синтаксис, описывающий схему, то есть структуру документа. Безусловно, стандарт XML содержит еще ряд других элементов, я описал наиболее употребительные для случаев хранения данных.\nВотличие от JSON XML гораздо более строг и не допускает произвольной вложенности и комбинирования элементов. Например, для одного тега не допускаются разные наборы элементов. Не допускается хранение XML в виде значения атрибута (конечно, так можно сделать, ведь атрибут является текстовым значением, но это очень плохой стиль), в случае коллекции всем ее элементам необходимо иметь совершенно одинаковую схему, то есть набор тегов и атрибутов должен быть одинаков для каждого элемента. В отличие от JSON практически все основные БД поддерживают работу с XML как с самостоятельным типом данных, поскольку он достаточно строг и однозначен. Поддержка в этом случае состоит в предоставлении встроенных средств сериализации таблиц в XML и обратно. Кроме того, XML широко применяется для создания языков описания разметки гипертекстовых документов, при построении пользовательского интерфейса (HTML, XAML, RDLC и пр.).\nНиже представлены достоинства XML как средства хранения информации:\n• строгость синтаксиса существенно упрощает построение систем сериализации/десериализации;\n• этот формат широко поддерживается различными БД как встроенный тип данных;\n• можно выполнить простое прямое преобразование в другие языки, в том числе в языки разметки графических элементов (например, HTML);\n• существуют специальные расширения языка, описывающие различного\nрода преобразования и трансформации (XSLT);\n\n• имеется стандартный способ построения запроса к элементу или выборки элементов (XPath, XQuery).\nК недостатком данного формата можно отнести то, что он гораздо более «многословен», чем JSON, поскольку содержит больше чисто синтаксических символов.\nОблачное хранилище Microsoft Azure Storage\nРассмотрим, как реализовано облачное хранилище, на примере Microsoft Azure Storage. Оно состоит из четырех сервисов: BLOB Storage, Queue Storage, Table Storage и File Storage\nВиды сервисов Azure Storage Account\nНепосредственно хранение информации осуществляется в сервисах BLOB, Table и File Storage. Queue Storage — это облачный сервис обмена сообщениями и синхронизации распределенных приложений. Сервис Table Storage — база данных NoSQL типа «ключ — значение», которая будет подробно описана далее в книге. Пока же сосредоточимся на Azure File и BLOB Storage.\nРассмотрим подробнее, как создавать Azure Storage Account и управлять с вебпортала. Прежде всего необходимо нажать ссылку добавления новых ресурсов Azure, расположенную в левом верхнем углу, — + New. Затем в открывшемся окне выбрать последовательно Storage — Storage account.\n  \nДобавление нового Storage account через веб-портал После нажатия ссылки Storage account откроется форма настройки\nStorage Account.\nФорма настройки Storage Account\nВ ней доступны следующие конфигурации.\n• Имя аккаунта (поле Name) будет частью URL аккаунта\n<Name>.core.windows.net,\n• потому правила наименования ресурсов точно такие же, как и в случае\nименования ресурсов, доступных по URL.\n• Тип модели развертывания ресурсов (deployment model) — выбираем Resource Manager, чтобы иметь возможность добавить аккаунт к общей ресурсной группе PockerRumExample.\n• Тип Storage (Account kind) — Storage (general purpose v1). Помимо этого, доступны типы Storage (general purpose v2) и BLOB.\n• Выбираем уровень производительности (Performance) — Standard. В зависимости от выбранного уровня производительность операций чтениязаписи будет отличаться. Наиболее высоким уровнем будет обладать комбинация Account kind = BLOB, Performance = Premium.\n \nДля premiumуровня в качестве физических устройств хранения выступают SSDдиски.\n• В качестве режима репликации (Replication) выбираем LocallyRedundant storage (LRS). Эта опция означает, что информация, хранящаяся в Storage Account, физически реплицируется три раза, но в пределах одного датацентра. Помимо LRS, доступны различные режимы географической репликации в разных датацентрах в пределах одной зоны (ZRS) или среди нескольких различных зон (GRS и ReadOnlyGRS — репликация по различным географическим зонам с репликой, доступной только для чтения). Все эти режимы различаются по стоимости и уровню надежности и доступности, что позволяет реализовать облачные хранилища, отвечающие различным наборам требований.\nПосле создания Storage Account доступна следующая панель мониторинга и управления\nОбщая панель мониторинга и управления Storage Account\nНа этой панели доступны все четыре сервиса, входящие в состав Storage Account: Blob, File, Table и Queue. Далее в главе рассмотрим Blob Storage и File Storage. Сервис Table Storage представляет собой нереляционную базу данных, а Queue предназначен для синхронизации сервисов «потребитель — производитель».\n ",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 4.\nКратко рассмотрим возможности конфигурирования Storage Account в целом. Прежде всего, доступен File Explorer — бесплатная программа от Microsoft, позволяющая просматривать содержимое всех сервисов всех аккаунтов хранения.\nНа рисунке можно увидеть результат выборки программы из хранилища Table Storage (таблица events), которая содержит тестовые данные телеметрии, полученные путем приема сообщений через сервисы концентратора EventHub и потоковой аналитики Stream Analytics Job).\n Внешний вид программы File Explorer от Microsoft\nЧтобы обеспечить доступ к аккаунту извне, необходимо знать ключи доступа и URL, которые доступны на вкладке Access Keys\nКлючи доступа и строки подключения к Storage Account\n \nДве пары ключей или строк подключения нужны для обеспечения «бесшовного» обновления ключей. Для этого первоначально используется ключ key1, затем клиенты переключаются на ключ key2, а ключ key1 обновляется. После этого клиенты переключаются на новый ключ key1, а ключ key2 обновляется.\nСтраница конфигурирования аккаунта в целом позволяет менять тип аккаунта (Account kind), уровень производительности (Performance).\n Вкладка конфигурирования типа аккаунта, уровня производительности и репликации\nКподобным манипуляциям следует отнестись внимательно, поскольку каждый уровень имеет различную стоимость, а также переключение между ними может занять время, если в аккаунте много данных.\nСледующий важный конфигурируемый параметр — Shared access signature (SAS)\n \nВкладка конфигурирования типа аккаунта, уровня производительности и репликации\nКонцепция SAS состоит в том, что к объектам, расположенным в Storage Account, можно предоставить прямой доступ для скачивания — с помощью URL, который содержит ряд ограничивающих параметров, а именно: время жизни ссылки и ограничения IPадресов, с которых доступен ресурс. Эти параметры подписываются ключом аккаунта, и данная подпись добавляется в конец URL, по которому данный ресурс может быть доступен.\nТеперь подробнее познакомимся с отдельными сервисами хранения файлов Storage Account.\nСервис Azure BLOB Storage предназначен для хранения различных файлов, потому и называется хранилищем больших двоичных объектов (Binary Large OBjects Storage, BLOB). Двоичные объекты могут храниться в нем как непосредственно, так и будучи размещенными в контейнерах (не путайте с Dockerконтейнерами, речь идет о контейнерах BLOB Storage) или на виртуальных дисках, тоже расположенных в BLOB Storage Account. Чтобы прояснить ситуацию, рассмотрим схему\nСтруктура вложенности объектов в хранилище BLOB\nАккаунт Azure может содержать один или несколько Storage-аккаунтов. Каждый такой аккаунт способен непосредственно хранить файлы, виртуальные жесткие диски и контейнеры BLOB. Последние, в свою очередь, тоже могут включать файлы, виртуальные жесткие диски и другие контейнеры. Таким образом, c помощью контейнеров BLOB реализована иерархическая структура организации файлов.\nНа самом деле в BLOB Storage могут храниться любые файлы: текстовые, двоичные и др., но для разных типов хранимых объектов требуется разный тип BLOB. Всего есть три типа BLOB: Page BLOB, Block BLOB и Append BLOB.\nPage BLOB — бинарный объект со страничной организацией памяти. Этот тип используется только для размещения виртуальных жестких дисков виртуальных машин.\n \nBlock BLOB — BLOB с блочной организацией памяти, служащий для хранения всех видов файлов (кроме VHD), включая контейнеры BLOB. Это основной тип хранения файлов в BLOB Storage обычных файлов.\nAppend Block — BLOB с блочной организацией памяти, представляющий собой текстовый файл, размещенный в Azure Storage и допускающий добавление новой записи в конец файла. В остальных типах BLOB файлы нередактируемые, то есть, чтобы отредактировать файл, его необходимо скачать, открыть, отредактировать и закачать обратно. Понятно, что эти действия сопряжены с большими трудностями при работе с файлами логов. В то же время Append Block как раз оптимизирован для сценариев прямой записи логов.\nВсе объекты, расположенные в BLOB Storage, могут быть доступны через вебпотрал, с помощью SDK, команд расширения командной оболочки, а также по прямой ссылке.\nДоступ к Storage-аккаунту с помощью веб-портала позволяет создавать файлы, контейнеры, просматривать список файлов, добавлять, удалять и загружать их, менять области видимости файлов (они могут быть общедоступными по ссылке, закрытыми для всех, кроме сервисов Azure). Интерфейс веб-портала удобен для работы с небольшим количеством файлов. Добавлять в облачное хранилище через веб-портал можно файлы не слишком большого размера. А вот файлы, загружаемые из облака, могут быть любого размера, допустимого в хранилище. Кроме того, для файлов можно открыть общий доступ, и они станут доступными для скачивания по ссылке. Доступ может быть открыт для файлов как в корневом каталоге, так и в контейнерах внутри хранилища.\nСпособы доступа к файлам в облачном хранилище BLOB\nэДля программного доступа облачный аккаунт содержит REST API, который, свою очередь, через SDK предоставляет гораздо большие возможности: синхронную и асинхронную загрузку и выгрузку, удаление, создание, добавление в конец Append BLOB и пр. Кроме того, через SDK можно создать временную ссылку на файл, то есть ссылку, становящуюся нерабочей через определенный промежуток времени.\nРассмотрим подробнее, как работать с хранилищем BLOB с помощью веб- портала. Чтобы перейти к хранилищу BLOB, необходимо нажать ссылку Blobs на общей панели аккаунта. В результате откроется вкладка\n \n Общая панель сервиса BLOB Storage\nДалее требуется добавить контейнеры. Для этого необходимо нажать ссылку + Container\nФорма создания нового контейнера\nВданной форме указано имя (свойство Name) — pokercomm; уровень доступа (Public access level) — Private (no anonymous access). Этот тип доступа подразумевает доступ не по прямой ссылке, а только с помощью ключей аккаунта с использованием SDK. Другие варианты типа доступа: Blob (anonymous read access for blobs only) — разрешает анонимный доступ только к файлам (BLOB); Container (anonymous read access for containers and blobs) — разрешен анонимный доступ к контейнерам и файлам. Вкладка созданного контейнера выглядит следующим образом\n  \nВкладка созданного контейнера\nЗагрузить файл в контейнер можно с помощью ссылки Upload. Свойства кон тейнера доступны по ссылке Container properties. Они включают в себя имя, адрес, статус, количество и суммарный размер BLOBобъектов, статус.\nВкладка свойств контейнера\nДля контейнера доступна настройка политики доступа через вкладку Access policy. Эта настройка позволяет организовать различный уровень доступа к различным контейнерам и объектам BLOB.\nВкладка настройки политики доступа\nПомимо упомянутых настроек, для BLOB доступен ряд других.\n  \n Различные настраиваемые сервисы Blob Storage\nСамые важные параметры:\n• CORS (сrossorigin resource sharing — совместное использование ресурсов между разными источниками) — свойство, обеспечивающее доступ к ресурсам BLOB из другого домена;\n• Custom domain — конфигурирование DNSзаписей CNAME в целях указания домена пользователя, в дополнение к домену аккаунту Azure Storage;\n• Encryption — шифрование объектов в хранилище;\n• Azure CDN — конфигурирование Azure Content Delivery Network, которая служит для хранения часто используемого контента из хранилища BLOB с анонимным доступом.\nКак уже было сказано, доступ к объектам в BLOB Storage возможен по прямой ссылке URL или с помощью интерфейса REST API (напрямую либо через SDK). Данный способ хранения обеспечивает самый быстрый доступ (минимальное время загрузки/выгрузки), но требует применения специальных программных клиентов, взаимодействующих с этими API. Последнее условие может помешать существующим приложениям большого масштаба мигрировать в облако.\nКроме того, в ряде случаев нужно создать облачное хранилище, которое должно быть доступно из виртуальной машины без всяких «самописных» программных клиентов. Для этого в хранилище BLOB можно разместить\n\nвиртуальный жесткий диск (или набор дисков для RAIDмассива) и примонтировать его к виртуальной машине.\nПреимущество такого решения состоит в том, что данные из виртуальной машины могут быть доступны точно таким же образом, как и c физического диска, подключенного к ней. Недостаток такого способа заключается в его низкой масштабируемости (верхний предел размера ограничен на уровне, определяемом операционной системой виртуальной машины и типом ее устройства ввода-вывода), дороговизне, а также в недоступности файлов извне по прямой ссылке.\nСкорость доступа к файлам тоже определяется конфигурацией топологии соединения дисков в массив RAID и в ряде ситуаций существенно ниже, чем в случае BLOB. Кроме того, один VHD может быть примонтирован строго к одной виртуальной машине. Невозможный или затрудненный доступ к файлам извне, также сложности при конфигурировании совместного доступа к файлам из нескольких виртуальных машин значительно ограничивают возможности такого хранилища в облачных архитектурах для оперирования больших данных. Чтобы преодолеть эти ограничения и одновременно обеспечить работу с файлами стандартными средствами операционных систем и программных библиотек вводавывода, следует использовать облачное файловое хранилище Azure File Storage\nСпособы доступа к облачному файловому хранилищу Azure File Storage\nИтак, сервис Azure File Storage составляет часть Azure Storage Account. Каждый Storage Account может включать в себя одну или несколько шар (share). Чтобы создать новую шару, необходимо c общей панели (см. рис. 4.7) добавить вкладку, нажав + File share. В появившейся форме (рис. 4.21) указывается имя шары (Name) — в нашем примере это pokerfileshare — и\n \nее размер (Quota), в данном случае 10 Гбайт. Размер каждой шары (квота) может изменяться (с помощью вебпортала, SDKуправления облачными ресурсами или Azure CLI) и способна достигать 5 Тбайт.\nФорма добавления новой файловой шары\nКлючевым отличием шары от контейнера является то, что для нее устанавливается квота — верхний предел размера. Эта шара представляет собой корневой каталог, который доступен по протоколу SMB 3.0 и может быть примонтирован\nквиртуальным машинам в том же аккаунте Azure и в том же регионе, что и Azure File Storage.\nНаиболее важными опциями конфигурирования является возможность подключения (Connect) к виртуальной машине и создание мгновенного снимка (View snapshot).\n  Доступные опции конфигурирования файловой шары\n\nЧтобы подключить шару File Storage к виртуальной машине, в оболочке виртуальной машины нужно выполнить команду монтирования сетевого диска. Эта команда может быть получена напрямую на вебпортале после щелчка на ссылке Connect\nВкладка Connect Azure File Storage\nДостоинства файлового хранилища представлены ниже.\n• Возможность прямой миграции файловой системы из локального хранилища в облачное. Будет полностью сохранена иерархия этой системы (вероятные ограничения на символьную длину пути и глубину вложенности каталогов см. в документации).\n• Простота взаимодействия из всех программных продуктов, реализующих стандартные интерфейсы ввода-вывода. Не требуется никаких модификаций кода программ, они могут быть устаревшими и все равно будут работать с Azure File Storage, поскольку это хранилище монтируется к основной файловой системе виртуальной машины на уровне операционной системы.\n \n• Возможен просмотр списка файлов с помощью стандартных средств Windows или Linux.\nНедостатки файлового хранилища:\n• ограниченный размер файловой шары (5 Тбайт) и одного файла (1 Тбайт); более высокая по сравнению с BLOB Storage цена за гигабайт;\n• меньшая по сравнению с BLOB Storage производительность операций чтения-записи.\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 5.\nВнастоящее время платформа Microsoft Azure позволяет использовать как виртуальные машины с готовыми предустановленными серверами SQL, так и РБДсервисы. К последним на настоящий момент можно отнести три: MySQL, PostgreSQL и Azure SQL.\nСамый технически зрелый и долгоживущий сервис — Azure SQL, представля ющий собой облачный сервис РБД на основе движка Microsoft SQL Server. В языке запросов Azure SQL реализовано подмножество функций TSQL. Экземпляры сервисов Azure SQL, являющиеся прямыми аналогами баз MSSQL, логически группируются в серверы Azure SQL Server. Каждый такой сервер должен располагать уникальным URL, учетными данными (имя пользователя и пароль), а также набором допустимых IPадресов, которые могут иметь доступ к нему (этот список формируется в фаерволе сервера и регулирует правила доступа к нему).\nФизически Azure SQL Server размещается в ЦОДе, расположенном в определенном географическом регионе, и в этом же регионе размещаются все экземпляры баз данных Azure SQL. Возможна также географическая репликация баз в несколько регионов по схеме PrimarySecondary или Primary — ReadOnly Replica (первичный сервер — реплика, доступная только для чтения). Это позволяет увеличить надежность и доступность баз.\nБлагодаря широкой поддержке TSQL Azure SQL предоставляет возможность прямой миграции баз данных из Microsoft SQL Server в Azure SQL. Конечно, прямая миграция из одного типа базы в другую — далеко не простая и не быстрая задача, но в данном случае это принципиально возможно и напрямую поддерживается с помощью специальных программ от Microsoft, Red Gate и др. Однако следует иметь в виду: указанное программное обеспечение в ряде случаев просто «вырезает» несовместимые объекты базы, не пытаясь их адаптировать. Вероятна такая ситуация: после такой миграции БД «поднялась», но счастливые разработчики могут «внезапно» недосчитаться ряда объектов базы в Azure SQL. (Я однажды имел неудовольствие обнаружить, что при миграции базы из нее исчезли бо'льшая часть триггеров и ряд хранимых процедур, поскольку содержали обращения к объектам другой БД, что в случае Azure SQL требует иного подхода.)\nКаждый экземпляр баз данных имеет определенный ценовой уровень (pricing tier), который характеризуется производительностью, ограничениями по размеру, количеству точек восстановления и возможностями репликации. Все потенциальные ценовые уровни разделены на базовые (Basic), стандартные (Standard) и премиум (Premium). Самое главное различие ценовых уровней проявляется в разном значении DTU — обобщенного параметра, характеризующего производительность БД.\n\nDTU (eDTU, elastic DTU) — интегральная характеристика производительности БД, включающая в себя показатели производительности центрального процессора, памяти, устройств вводавывода и сетевого интерфейса. DTU определяет своего рода объем, который может занимать производительность Azure SQL. Если в данный момент в базе выполняется запрос, то он потребляет определенное количество ресурсов, занимающих часть этого разрешенного объема\nГрафическое представление DTU\nПри этом следует иметь в виду, что ограничивается не только объем, но и конкретные значения каждого из показателей (то есть не получится «обменять» крайне малое использование CPU на крайне большое значение памяти). Это ограничение проявляется, в частности, в том, что один «кривой» запрос может вызвать переиспользование одного из ресурсов и в итоге «подвесить» всю базу (экземпляр сервиса Azure SQL Database, но не Azure SQL Server), и точно такой же запрос будет работать нормально в традиционной БД Microsoft SQL Server. Это происходит потому, то в Azure SQL срабатывает механизм защиты ресурсов базы от чрезмерного применения, который вынуждает процесс, вызвавший запрос, отключиться по таймауту, освободив тем самым ресурсы БД.\nИтак, концептуально Azure SQL состоят из экземпляров Azure SQL Database, являющихся собственно реляционными хранилищами информации с определенными значениями размера и максимального DTU; экземпляра Azure SQL Server, который группирует базы Azure SQL Database, обеспечивая им общую строку соединения (connection string); правил доступа, прописанных в фаерволе; эластичного пула ресурсов (elastic database pool); секционированной базой данных (sharded database) и реляционного хранилища данных (Azure SQL DWH). Рассмотрим эти сервисы по порядку.\n \nЧтобы создать экземпляр Azure SQL, необходимо сначала в левом верхнем углу портала выбрать ссылку +New и затем в появившемся окне щелкнуть на ссылке SQL Database. После этого откроется страница с формой конфигурирования сервиса\nНачальная страница конфигурирования сервиса Azure SQL\nВкачестве имени базы данных (Database name) указываем Test. Выбираем существующую группу ресурсов (Resource group) — Example.\nВ качестве источника базы (Select source) указываем Blank database. Поскольку у нас нет сервера БД, его следует создать. Для этого указываем имя сервера (Server name) — pokerexample, учетные данные (Server admin login и Password) пользователя администратора и его местоположение (Location) — Central US.\nДалее следует выбрать ценовой уровень (Pricing tier). На уровне Basic можно только изменить размер базы данных. Для других уровней (Standard, Premium) доступна более гранулярная настройка размера и производительности, включая настройку подуровней.\n \n Настройка ценового уровня базы данных\nПосле создания в сервисе Azure SQL доступны различные дополнительные сервисы, рассмотрим наиболее важные и полезные из них (рис. 5.6).\nОбщая страница управления и мониторинга Azure SQL\nЧтобы получить доступ к базе данных извне, необходимо прежде всего сконфигурировать фаервол. Ссылка доступа к ней находится в верхней части портала (Set server firewall).\n \n Фаервол сервера Azure SQL\nЧтобы получить доступ к БД извне, нужно получить строку подключения\nсучетными данными пользователя. Для этого следует нажать ссылку Connecting string на левой навигационной панели заглавной страницы (рис. 5.8).\nПолучение строки подключения для базы данных\nСамые главные показатели мониторинга БД — DTU и размер базы. Мониторинг использования DTU позволяет анализировать закономерности в функционировании БД, например суточные пики и провалы активности или загруженность базы по дням недели. Этот мониторинг очень важен не сам по\n \nсебе, а ввиду того, что позволяет определить, является текущий ценовой уровень оптимальным или необходимо его изменить. Если конкретнее, то уровень применения DTU должен,\nсодной стороны, быть таким, чтобы база была максимально загружена, а с другой — чтобы случайные пики не вызывали 100процентного использования ресурсов DTU. И здесь весьма уместен мониторинг DTU.\nОднако возможен ряд вариантов. Если БД в целом равномерно загружена без ярко выраженных пиков и провалов, то рекомендуется ценовой уровень, допускающий использование около 70–80 % DTU. Теперь рассмотрим случай, когда база данных имеет ярко выраженные и узкие пики (spikes). Подобная ситуация неприятна тем, что ради обеспечения достаточной производительности БД необходим ценовой уровень, слегка превышающий уровень этих пиков. Но если такие пики достаточно редки, то ресурсы базы задействуются достаточно нерационально: большую часть времени она работает существенно недогруженной (с малым уровнем DTU), а во время пиков — зачастую перегруженной (DTU близка к 100 %). Эта нерациональность в конечном итоге приводит к неоправданно большому счету за облачные ресурсы. Логичнее и эффективнее всего в подобном случае использовать сервис Query Performance Insight, постараться определить, какие запросы в базе приводят к таким пикам, и попытаться устранить их за счет оптимизации запросов, список которых выдает упомянутый сервис.\nВнешний вид вкладки сервиса Query Performance Insight\nНаряду с этим доступна возможность автоматической настройки производительности с помощью сервиса Automatic Tuning.\nАвтоматическая настройка производительности состоит в динамическом добавлении/удалении индексов, а также перекомпилировании плана исполнения. Каждый акт срабатывания автоматической настройки\n \nотображается в логах. Помимо этого, Microsoft предоставляет целый ряд сервисов подстройки и автоматической оптимизации базы данных (Performance overview, Performance recommendations и пр.), на которые следует обратить внимание при реальном функционировании БД.\nТеперь рассмотрим очень важный случай работы нескольких баз данных Azure SQL на одном сервере. В реальных проектах нагрузка на базы может быть неравномерной не только изза наличия пиков, обусловленных проблемами с запросами, но и изза периодических пиков запросов пользователей. Если им соответствует строгая периодичность (скажем, большая нагрузка днем и маленькая ночью), то можно настроить автоматическое масштабирование базы данных по расписанию, например, с помощью сервиса Azure Automation.\nЕще один случай — некоррелированные запросы могут послужить причиной использования Azure Elastic Database Pool, который встроен в состав Azure SQL Server и служит для объединения баз в один пул и назначения всем им общих разделяемых ресурсов сервера. Рассмотрим ситуацию более детально на примере SaaS приложения, созданного для оказания неких услуг зарегистрированным в нем пользователям (допустим, это облачная CRM система). Каждому из пользователей выделяется своя БД определенного уровня производительности, который выражается конкретным значением DTU. Каждый уровень имеет определенную месячную стоимость, которая в конечном итоге скажется на прибыли владельца SaaS: она будет равна суммарной месячной плате всех пользователей за вычетом расходов на облачные сервисы, лежащие в основе архитектуры системы. Теперь предположим, что активность пользователей, выражающаяся в применении DTU каждой базы, носит некоррелированный характер, то есть пользователи нагружают свои базы в различные случайные временные промежутки. Это приводит к появлению моментов времени, когда применение DTU базы конкретного пользователя очень мало, и моментов, когда оно велико (рис. 5.13).\nБазы данных, подходящие для объединения в Elastic Pool\n \nТаким образом, в момент малого использования DTU базы простаивают — но Azure взимает плату за них. А если взять несколько БД, у которых интервалы активности одних баз совпадают с интервалами недогруженности других, объединить их в общий пул БД и распределить общие ресурсы DTU между базами таким образом, чтобы суммарная нагрузка была равномерной во времени? (Плата за ценовой уровень при этом будет существенно меньше, чем суммарная плата за все базы данных.) Именно такая идея лежит в основе сервиса Azure Elastic Database Pool. Вдобавок существует ряд рекомендаций относительно выбора оптимального ценового уровня. Выбор eDTU (elastic DTU) производится с помощью приближенного соотношения:\nСуммарное eDTU = MAX(<Общее количество баз данных × Среднее использование eDTU>, <Количество баз данных одновременно достигающих пиковых нагрузок × Пиковый уровень DTU базы>)\nДалее для определения минимального объема хранилища необходимо просуммировать объемы всех отдельных баз данных. Затем следует выбрать ценовой уровень, дающий максимальное значение eDTU, полученное исходя из формулы и размера хранилища.\nЧтобы добавить Elastic Database Pool, нужно выйти на начальную страницу сервера. Для добавления нового Elastic Pool на этой странице следует нажать на ссылку + Elastic pool\n \n  Добавление баз данных в пул и удаление их из него может происходить динамически без остановки и передеплоивания баз данных. Для этого в панели управления есть графики мониторинга и сервис выдачи рекомендаций по включению баз данных в пул и исключению из него.\nКроме того, существует сервис Elastic Query, позволяющий выполнять задания, общие для всех баз данных из Elastic Database Pool. Суть его в том, что группа баз объединяется и управляется централизованно с общего специализированного головного сервера ВМ. Взаимодействуя с этим сервером программно или через Azure Portal, можно управлять всеми подключенными БД с помощью создания заданий (jobs).\n\nСервис Elastic Database jobs содержит следующие компоненты:\n• головной сервер, размещаемый на экземпляре Azure Cloud Service Worker Role. По сути, это специализированное ПО, размещаемое (по состоянию на настоящее время) на виртуальной машине Azure Cloud Service. Для обеспечения высокой доступности рекомендуется создавать\nминимум два экземпляра ВМ;\n• управляющую (головную) БД — экземпляр Azure SQL, служащий для\nхранения метаданных всех подключенных баз;\n• экземпляр сервиса Azure Service Bus, служащий для объединения и\nсинхронизации всех компонентов;\n• экземпляр облачного хранилища Azure Storage Account, служащий для\nхранения журналов всей системы.\nНастройка, использование и администрирование всей этой системы — довольно сложная тема и в данной книге подробно рассматриваться не будет. Очень важное свойство Elastic Database Pool — возможность реализовывать\nсценарии разбиения одной большой БД на меньшие, но выполнять запросы в разделенной на фрагменты (shard) базе так, будто это одна монолитная база (рис. 5.17). Рассмотрим данный механизм подробнее. Нужда во фрагментации БД появляется\nОбщая структура фрагментированной базы данных,построенной на основе Elastic Database Pool\nвслучае, когда ее размер становится чрезмерно большим для размещения на одном экземпляре Azure SQL (в настоящее время это более 1 Тбайт для ценового уровня Premium). Конечно, можно разбить большую БД на меньшие логически, проведя анализ ее структуры (схемы). Как уже указывалось ранее, Azure SQL Server — логическая группировка экземпляров Azure SQL Database, а не физическое объединение на одном сервере. И прямые запросы к объектам одной базы из другой возможны, только если объекты являются внешними таблицами или базы объединены в Elastic Database Pool и используются запросы Elastic Database Query или транз акции Elastic Transactions. В таком\n \nслучае необходимо применить фрагментацию (sharding) базы данных. По сути, это горизонтальное масштабирование, отличное от вертикального — увеличения размера базы в масштабе CPU, оперативной памяти, IOPs и пр. Разделение одного большого хранилища на несколько хранилищ меньшего масштаба и их параллельная обработка с последующим сложением результатов последней — ключевая концепция всех технологий обработки больших данных, с которой мы будем не раз встречаться далее. Фрагментированная БД концептуально во многом близка к реляционному хранилищу данных DWH, но требует больших усилий при создании и использовании.\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 6.\nОбщий обзор специализированных облачных хранилищ больших данных\nВпредыдущих главах мы рассмотрели облачные хранилища файлов и СУБД различного типа: как реляционные, так и нереляционные. Это достаточно традиционные сервисы, хорошо подходящие для создания информационных систем широкого назначения, включая масштабируемое файловое хранилище, базу данных телеметрии IoT устройств, масштабируемое хранилище логов и т. д. Но в то же время у всех описанных сервисов есть ряд существенных ограничений. Рассмотрим их по порядку.\nОблачные хранилища файлов допускают хранение огромного количества файлов, суммарный объем которых исчисляется многими терабайтами и даже петабайтами. Но в хранилищах эти файлы просто хранятся, а как быть, если нужно провести общее исследование информации в них? Например, для многих сотен файлов логов суммарным объемом несколько гигабайт сгруппировать IPадреса запросов, чтобы определить странуисточник последних. Или совершить более сложные исследования логов с многих сотен серверов для определения корреляции в событиях, узких мест производительности и пр. Для выполнения подобных действий необходим сторонний сервис, которому это под силу, но он должен иметь доступ к каждому из логов. В принципе, у AWS есть такие сервисы: AWS Athena и AWS RedShift Spectrum, и они специально предназначены для анализа информации в файлах с помощью SQLподобного синтаксиса (более подробно он будет описан далее).\nЕсли задачу анализа лог-файлов общего назначения можно считать решенной, то для более сложных случаев указанные сервисы недостаточно приспособлены. Как быть, если логфайлов очень много (терабайты) и необходимо получить периодический отчет, применить алгоритмы машинного обучения или выполнить совместный анализ информации из файлового хранилища и, скажем, реляционной базы данных? Для решения указанных проблем нужно переместить эти файлы\nвхранилища, допускающие подобные манипуляции. Но если требуется применить машинное обучение или выполнить пакетный анализ огромного количества данных, то нужно использовать HDFSсовместимое хранилище. Вообще, как правило, реляционное хранилище данных — последнее звено в цепочке системы анализа больших данных.\nЭта цепочка включает:\n• прием потоков данных;\n\n• хранение сырых данных в нереляционном хранилище;\n• копирование и агрегацию данных в HDFSсовместимое хранилище Data\nLake;\n• выполнение интеллектуального анализа данных;\n• помещение результата в реляционное хранилище.\nРеляционное хранилище обеспечивает доступ к информации для сервисов бизнес-аналитики. Но в моей реальной практике был пример архитектуры, в которой было два оконечных хранилища: DWH и Data Lake — и каждое из них выполняет свою задачу.\nНаполнение хранилища DataLake данными из иных облачных хранилищ и доступ сторонних сервисов к нему\nВ этом случае можно применить сервисы «из мира» Hadoop (MapReduce, Spark, Pig, Hive и др.) или облачных средств аналитики HDFSсовместимых хранилищ (Azure Data Lake Analytics). Такой подход именуется Data Lake («озеро данных»). Data Lake является хранилищем структурированных, неструктурированных и частично структурированных файлов различного типа и совместно с сервисами экосистемы Hadoop образует «склад данных», который отличается от традиционного SQL DWH, но обладает гораздо большими возможностями, предоставляемыми средствами экосистемы Hadoop.\nData Lake позволяет применять ELT, то есть сначала загрузить файл (в ряде случаев просто копировать), а потом, если надо, преобразовать.\nКроме того, в Data Lake все данные хранятся как файлы в специализированной файловой системе HDFS — каждый файл имеет имя, путь, и эти параметры напрямую фигурируют в запросах обработки данных. В SQL DWH же, как и в любой СУБД, хранение информации полностью отделено от ее логического представления.\n \nИтак, опишем кратко HDFS и поясним, почему она так важна для построения распределенных масштабируемых хранилищ, допускающих массивнопараллельную обработку хранящейся в них информации. HDFS (Hadoop Distributed File System) — распределенная файловая система Hadoop, предназначенная для хранения файлов, которые поблочно распределяются между узлами кластера, лежащего в основе HDFS. Каждый блок (за исключением последнего блока файла, служебного) можно расположить в одном или нескольких узлах одновременно, что определяется коэффициентом репликации. Таким образом, коэффициент определяет количество узлов, в которых может быть размещен блок, и является параметром, настраиваемым на уровне файла. Наличие репликации, а по сути, избыточности данных обеспечивает отказоустойчивость кластера по отношению к отказам отдельных узлов.\nКластер HDFS состоит из узлов двух типов: головного, или узла имен (name node), который хранит все метаданные о файлах и репликации их блоков между узлами, а также из узлов данных (data node), в которых хранятся блоки файлов\nВ отличие от обычных файловых систем (например, NTFS) HDFS позволяет только записывать и удалять файл, но не допускает его модификацию. Записывать файл в одно и то же время может только один процесс. При этом организация файлов — традиционная иерархическая с корневым каталогом и набором вложенных каталогов.\nКаждый узел, будь то узел имен или узел данных, имеет доступ извне через командную строку, а также содержит вебсервер, который обеспечивает доступ к вебпорталу, отображающему статус узла и состав файловой системы.\nАрхитектура HDFS\nТакая файловая система лежит в основе большинства сервисов пакетного интерактивного анализа, сервисов машинного обучения из экосистемы Hadoop, а также составляет основу хранилища типа Data Lake. Рассмотрим подробнее облачные сервисы, реализующие концепции Data Lake.\n \nAzure Data Lake Store\nAzure Data Lake Store представляет собой масштабируемое хранилище данных для целей массивнопараллельной обработки как встроенными сервисами анализа (Azure Data Lake Analytics), так и сервисами экосистемы Hadoop, предоставля емыми сервисами HDInsight. Доступ из кластера HDInsight возможен с помощью REST API, совместимых со стандартом WebHDFS. В отличие от Azure Storage для Azure Data Lake Storage не указываются верхние пределы размеров и количества файлов. Отдельные файлы могут иметь размеры от килобайта до петабайта и сохраняются в виде реплицированных копий, разделенных по кластеру.\nОчень важное и полезное свойство этого сервиса — можно хранить файл том же формате, который использовался при его создании: CSV, LOG и пр. В этом- то и прелесть концепции Data Lake: хранение и анализ файлов без преобразования формата, но с максимально возможной производительностью.\nТеперь рассмотрим вопросы безопасности и защиты данных в сервисе Azure Data Lake Store. Прежде всего доступно шифрование данных. Кроме того, имеется возможность интеграции этого хранилища с Azure Active Directory (AD) для обеспечения управления учетными данными и контроля доступа к ресурсам (аутентификация). Интеграция с AD позволяет использовать все преимущества Azure AD, а именно: многофакторную аутентификацию, контроль доступа на основе ролей (rolebased access control) и мониторинг доступа к защищаемым ресурсам. Вообще же, Azure Data Lake Storage поддерживает протокол OAuth 2.0, что позволяет (по крайней мере теоретически) применять отличный от Azure AD сторонний провайдер, поддерживающий OAuth 2.0. Следующая опция Data Lake Store — поддержка прав доступа к файлам и каталогам на основе стандартов POSIX, доступных с помощью протокола WebHDFS.\nВ качестве внутренней файловой системы в этом сервисе может быть использована Azure Data Lake Store File System, являющаяся адаптацией HDFS; в последней объекты адресуются с помощью префикса adl://.\nРассмотрим на практике, как создавать экземпляр Azure Data Lake Store в вебпортале. Прежде всего в строке поиска доступных ресурсов следует написать Data Lake Store и в появившейся форме (рис. 8.3) нажать кнопку Create (Создать). Это приведет к открытию формы, показанной на рис. 8.4. Здесь нужно указать имя (Name), выбрать ресурсную группу (Resource group), местоположение (Location) и тип шифрования (Encryption).\nДоступны три типа шифрования: Do not enable encryption (Не шифровать данные), Use keys, managed by Data Lake Store (Использовать ключи, управляемые Azure Data Lake Store) и Use keys from your own Key Vault (Применить ключи, управляемые сервисом Key V ault и поставляемые пользователем). Особняком стоит настройка моделей оплаты — Pricing package.\n\n Форма конфигурирования Azure Data Lake Store\nГрафический интерфейс этого сервиса не так интересен, как его возможности. Укажу лишь, что оперировать данными на веб-портале можно после нажатия на ссылку Data explorer (Обозреватель данных)\nДля того чтобы сервисы аналитики могли проанализировать все эти данные, их нужно поместить в специализированное централизованное хранилище, допускающее такой анализ. Традиционно таковым является реляционное хранилище DWH. Оно позволяет применять эффективные высокопроизводительные запросы к данным из разных источников, приведенным к единообразной форме таблицы DWH. Такой подход копирования данных из разных источников называется ETL (Extract Transform Load — «извлечение, преобразование, загрузка»). Данные надо извлечь из источника, преобразовать (отфильтровать, агрегировать, извлечь подмножество полей) и загрузить в центральное хранилище.\nНапример, система мониторинга электроэнергии, в которой первичные данные (например, величины конкретных отсчетов) хранятся сначала втаблице «ключ — значение». Затем эту информацию требуется агрегировать (подсчитать энергопотребление для каждого потребителя за установленный временной промежуток) и поместить ее в реляционное хранилище. В то же время данные пользователей, данные, связанные с финансовыми аспектами (например, данные CRM), хранятся в реляционных БД. Чтобы провести анализ данных из нереляционных БД, связав их с конкретным пользователем или клиентом, необходимо скопировать в DWH также и эти данные.\n\nДопустим, требуется провести более сложный интеллектуальный анализ данных, например связанный с анализом подозрительных шагов или действий в игровой системе. Для этого, помимо данных из хранилища игровых событий, реляционных данных о пользователях, необходимо использовать данные логов приложений. В подобном случае наиболее подходящий вариант — применение нереляционного хранилища данных, такого как Data Lake. В этом случае файлы логов просто копируются, как и данные из нереляционной и реляционной баз данных, а преобразование выполняется уже после копирования. Такой подход называется ELT (Extract Load Transform — «извлечение, загрузка, преобразование»). В подобной ситуации необходимо использовать Data Lake, поскольку в настоящее время самые мощные системы интеллектуального анализа данных представлены в основном сервисами из экосистемы Apache Hadoop, что требует HDFSсовместимого хранилища, коим и является Data Lake.\nТаким образом, очевидно, что сервисы копирования и трансформации данных, которые переносят их из одного хранилище в другое, подвергая преобразованию, — очень важная составляющая информационной системы. Выше уже упоминались два сервиса из этой области — Apache Sqoop и DistCp. Но они требуют кластерных решений типа Azure HDInsight и AWS EMS. Эти сервисы предоставляют «ядро» или «движок» копирования и трансформации, однако конвейер данных может состоять из многих этапов копирования и преобразования информации. Управление этим конвейером берут на себя облачные сервисы копирования и трансформации данных: Azure Data Factory, AWS Data Pipeline и AWS Glue.\nAzure Data Factory\nAzure Data Factory представляет собой сервис трансформации и копирования от Azure.\nИтак, Azure Data Factory — сервис прежде всего для управления процессами копирования и трансформации данных. Как правило, этот сервис состоит из следующих компонентов\n• Наборы данных (dataset) — конкретные поименованные представления данных.В их качестве могут выступать файлы, таблицы и пр. Наборы данных могут быть как входными, называемыми источниками (source), так и выходными, называемыми воронками (sink).\n• Подключенные серверы (linked servers) — наборы метаданных источников (адреса, учетные данные, протоколы и др.), на которых располагаются источники и приемники данных.\n• Наборы выполняемых заданий (activity) — одно или несколько выполняемых заданий по копированию или трансформации данных.\n\n• Конвейер (pipeline) — набор сервисов по обеспечению периодического выполнения, управления и мониторинга, логически соединенный с подключенными серверами, наборами данных и заданиями для выполнения конкретной задачи.\nАрхитектура сервиса Azure Data Factory\nТеперь рассмотрим, как все компоненты работают и взаимодействуют между собой. Прежде всего, сервис Azure Data Factory состоит из одного или нескольких конвейеров. Каждый конвейер реализует конкретную задачу ETL/ELT, для чего ему нужны источники и приемники данных, собственно список выполняемых заданий, триггеры (то есть условия начала выполнения) и набор входных параметров, подаваемых на вход.\nВнастоящее время в качестве источников данных Azure Data Dactory выступают хранилища, как облачные (в том числе и в иных облачных средах, например AWS), так и локальные: реляционные и нереляционные БД, файлы (текстовые, CSV, HTML, JSON и XMLтаблицы), хранящиеся в различных файловых системах и доступные по разным протоколам (FTP, SFTP, HDFS, AWS S3), реляционные и нереляционные хранилища данных, сервисы REST и SOAP (в том числе поддерживающие протокол OData), SAP, Dynamic CRM, Office 365 и др.\nВто же время количество приемников данных в значительной степени зависит от того, какой исполняемый режим (runtime) выбран для Azure Data Factory. По умолчанию конвейер управляется ресурсами Azure и для пользователя выглядит как бессерверный. Такой режим называется встроенным исполняемым режимом (integrated runtime, IR). В нем пользователю не требуется выполнять никаких действий по администрированию и мониторингу конвейера — все нужные вычислительные ресурсы Azure создает автоматически. Но в этом случае перенос информации ограничивается облачными источниками и приемниками данных, расположенными в публично доступных сетях.\nЧтобы преодолеть это ограничение, задействуется вариант режима внешнего хостинга (selfhosted) — размещения исполняемого модуля Azure\n \nData Factory на физическом сервере или виртуальной машине. В таком случае применимы все возможности по копированию и переносу данных между любыми источниками приемниками из доступного набора. Однако пользователь отвечает за установку и администрирование хоста для исполняемого модуля.\nВернемся к остальным компонентам Azure Data Factory. Важнейший и ключевой компонент — выполняемое задание (activity). Существуют три вида заданий: перенос данных, их трансформация и поток управления (control flow). Рассмотрим их по порядку.\nПеренос данных служит для копирования данных из источника в приемник без выполнения каких бы то ни было трансформаций, агрегаций и пр. Это простое копирование из источника в приемник. Оба — источник и приемник — должны быть представлены как наборы данных (data set) в подключенных серверах (linked server). Как раз для подобной активности мы и рассмотрели различные исполняемые режимы (IR, selfhosted).\nТрансформация данных подразумевает применение вычислительных ресурсов кластера, выделяемого по требованию (Azure HDInsight), или ресурсов, выделя емых пользователем. В настоящее время преобразовывать данные позволяют следующие технологии: HDInsight — Hive, Pig, Spark, Hadoop Streaming, MapReduce; Machine Learning; хранимая процедура в базе данных SQL или хранилище данных; USQL сценарий Azure Data Lake Analytics и пользовательская программа, написанная на .NET. При трансформации возможны два исполняемых режима: с выделением ресурсов по требованию (например, кластер Azure HDInsight) или на ресурсах, предоставляемых пользователем (на его виртуальных машинах). Как видим, преобразование в этом случае может применяться в сценариях ETL/ELT, для выполнения пакетных, а также периодических административных заданий (например, запуск хранимой процедуры в Azure DWH для пересчета индексов). И Azure Data Factory при этом играет роль дирижера (orchestrator).\nИнаконец, очень важным является поток управления, который служит для управления другими заданиями, используя условные, циклические и прочие управляющие заданияоператоры. В настоящее время поддерживаются следующие типы заданий:\n• задание условного логического оператора ветвления IF (IF condition activity); работает по тому же принципу, что и логический оператор if в языках программирования: если логическое условие является истинным (true), то выполняется соответствующее задание А, в противном случае выполняется задание Б;\n• задание условного цикла с предусловием Until (Until condition activity); представляет собой реализацию оператора повторения выполнения задания до тех пор, пока условие, ассоциированное с этим циклом, истинно или не будет достигнут таймаут;\n\n• задание по выполнению RESTзапроса (web activity);\n• циклическое задание с фиксированным количеством шагов (for each\nactivity) — оператор повторения задания определенное число раз;\n• задание ожидания (wait activity) обеспечивает задержку выполнения\nосновного задания на определенное время;\n• задание последовательного перебора (lookup activity) может служить для\nчтения и последовательного просмотра записей в таблицах и файлах из\nвнешних источников;\n• задание по получению метаданных (get metadata activity) используется\nдля получения метаданных от подключенных источников;\n• задание активации другого конвейера данных (execute pipeline activity).\nКроме того, Azure Data Factory содержит достаточно богатый доменноспецифичный «язык» из богатого набора операторов, переменных (определяемых пользователем), функций и возможность строить различные выражения из этих элементов, обеспечивая довольно гибкое управление заданиями. Все компоненты Azure Data Factory можно описать с помощью JSON, и для наиболее гибкого конфигурирования данного сервиса рекомендуется использовать именно JSON.\nТеперь рассмотрим вопрос запуска конвейера данных. Собственно, запуск включает передачу на вход конвейера параметров и активацию соответствующей конечной точки, будь то REST или точка запуска, определяемая в триггере конвейера. При этом каждый исполняемый конвейер получает уникальный идентификатор и становится доступным для средств мониторинга. Запустить конвейер можно с помощью:\n• команды Azure CLI или Azure PowerShell;\n• прямого RESTзапроса к конечной точке запуска;\n• триггера, определяемого в JSONфайле описания конвейера.\nОбратите внимание: в настоящий момент Azure Data Factory не поддерживает запуск по событию и прямую интеграцию, например, с Azure Function (по крайней мере на момент написания книги такой возможности нет), Azure Logic App и др., но можно построить архитектуру Event Driving косвенно, через активизацию конечной точки REST.\nЕдинственно поддерживаемый вид запуска, управляемый напрямую Azure Data Factory, — это триггер, запускаемый по расписанию. Чтобы быть точным, триггер — внутренний сервис Azure Data Factory, который определяет момент времени и периодичность запуска конвейера. Один триггер может служить для запуска нескольких конвейеров. И наоборот, один конвейер может быть запущен несколькими различными триггерами.\n\nТеперь рассмотрим, как создавать и конфигурировать сервис Azure Data Factory помощью вебпортала Azure. В качестве примера возьмем задачу копирования данных из таблицы Azure Table Storage в реляционную БД Azure SQL. Концептуально это ничем не отличается от задачи копирования информации в реляционное хранилище DWH, но по финансовым затратам гораздо ниже.\nИтак, сначала следует нажать ссылку добавления новых ресурсов вебпортала управления и ввести в строке поиска Data Factory.\nПосле нажатия кнопки Create (Создать) откроется форма конфигурирования сервиса. Здесь указываются имя сервиса (Name), подписка Azure (Subscription), ресурсная группа (Resource Group), версия (Version) и местоположение (Location).\nЗавершив создание этого сервиса, можно перейти на главную панель мониторинга).\nНо сама по себе эта панель интересна только с точки зрения мониторинга. Чтобы настроить Azure Data Factory, следует перейти по внешней ссылке Author & Monitor. В результате откроется внешняя страница конфигурирования вашего экземпляра Azure Data Factory\nФорма конфигурирования сервиса Azure Data Factory\nПрямо на ней имеются ссылки на создание конвейера данных (Create pipeline), копирование данных (Copy Data), настройку интеграции с SSIS (Configure SSIS Integration Runtime) и настройку интеграции с Git (Configure Git Repository). Пойдем длинным путем и создадим конвейер данных, выбрав ссылку Create pipeline. В результате увидим такую страницу\n \n Здесь мы видим все основные компоненты конвейера данных, включающие себя активности копирования (Data Flow — Copy), HDInsight, поток управления и пр. Но прежде всего необходимо настроить наборы данных. Для этого следует нажать на ссылку Connections в левом нижнем углу. Появится следующая форма\n Нас интересует источник данных — таблица Azure Table Storage. Для этого\nвстроке поиска следует написать Azure Table Storage, выбрать появившийся значок и нажать его. В результате появится форма конфигурирования\n\n  Форма конфигурирования подключенного сервера Azure Table Storage\nЗдесь указывается имя сервера и дается его описание (Description). Очень важный параметр — подключение через специальный исполняемый режим (Connect via integrated runtime). В данном случае следует выбрать режим по умолчанию (Default), поскольку этот подключенный сервис не требует настройки специализированного режима исполнения. Далее из подписки Azure (Azure subscription) нужно выбрать наш Azure Storage Account и проверить соединение с ним, нажав на ссылку Test connection. При успешной проверке соединения следует нажать кнопку Finish (Закончить).\nТочно так же конфигурируется подключенный сервер Azure SQL. Созданные подключенные серверы можно посмотреть на вкладке Connections.\nДалее следует перейти на вкладку pipeline1 и вручную перетянуть\nграфический компонент выполняемого задания DataFlow — Copy на общую панель конвейера. Нужно дать имя выполняемому заданию, установить таймаут, количество попыток повторений в случае отказа (Retry) и интервал между попытками.\nДалее необходимо сконфигурировать источник (Source) и приемник (Sink) данных. Перейдя на вкладку Source, следует нажать на ссылку + New, ввести в появившейся строке поиска SQL и сконфигурировать Azure Table Storage как источник данных.\n\n На этой панели нужно выбрать наш подключенный сервер и имя таблицы. Далее можно проверить соединение (Test connection) и просмотреть данные (Preview data).\nДалее в нашей базе данных SQL следует создать таблицу, которая будет хранить данные из Azure Table Storage, выполнив следующий сценарий в онлайн -редакторе или в стороннем приложении наподобие SQL Management Studio.\nЗатем эту таблицу можно подключить как приемник (Sink) таким же образом, как подключали источник\nКонфигурирование приемника данных — таблицы Azure SQL\nПосле этого следует перейти на вкладку отображения схем источника и приемника выполняемого задания и при необходимости выполнить согласование схем, выбрав требуемые параметры и указав их типы.\n \n Вкладка отображения схем источника и приемника\nЗатем, перейдя на вкладку конвейера, можно нажать на ссылку Test Run в верхней части вкладки. Если вы внимательно и правильно все\nсконфигурировали и настроили, то конвейер отработает успешно .\nПосле создания и тестирования нужно задеплоить шаблон (по сути, весь этот портал служит графической оберткой для создания JSONдокумента), нажав на ссылку Publish All, размещенную на общей панели конвейера. Затем можно создать триггер, который будет запускать этот конвейер в заданные промежутки времени. Для этого следует нажать на ссылку Trigger в верхней\nчасти панели конвейера.\n ",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 7.\nИсточники данных в AZURE DATALAKE\nИсточниками данных для AZURE DATALAKE могут служить различные системы, такие как: хранилища DWH, операционные системы, HTTP сервисы и др. Каждая из систем классифицируется по виду расположения, типу источника, принципу подключения.\nВиды расположения\nВиды расположения определяют уровень открытости источника данных по отношению к Datalake.\n   Вид источника\nПубличные сети\nAzure Virtual Machine\nЗакрытые сети\nМеханизм извлечения\nAutoResolve IntegrationRuntime Logic App\nWeb Request\nShared\nIntegration Runtime\nSelf Hosted Integration Runtime\nОписание\nСети имеющие открытый доступ. Доступ к данным этих сетей возможен из любого источника. Integration Runtime стандартный для извлечения данных в Azure Data Factory\nПриложения Logic App способные извлекать данные из публичных источников\nHTTP/HTTPS Web запрос к публичному сервису Виртуальные сервера расположенные в Azure\nIntegration Runtime предоставляемый в общее пользования глобальной AOH командой.\nIntegration Runtime устанавливаемый на стороне системы источника в закрытой сети.\nОписание\nSQL ориентированные базы данных\nИзвлечение с помощью Azure Data Factory через указание типа SQL источника\nДанные систем на внешнем хранилище. Как правило выгрузки из системы внутри корпоративной сети.\nДоступ к общей папке на стороне File Server Запрос к сервису\nДоступ к общей папке на стороне File Server Запрос к сервису\n                           Типы источников\nТип источника Базы данных\nФайлы данных систем\nМеханизм извлечения\nAzure Data Factory\nADF FTP/SFTP\nADF Server File Folder\nADF HTTP\nLogicApp FTP/SFTP LogicApp Server File Folder\nLogicApp HTTP\n                                Внешние FTP/SFTP/HTTP источники файлов\n       Файловые источники находящиеся вне корпоративной сети.\n \n   ADF FTP/SFTP\nADF HTTP Rest API LogicApp FTP/SFTP LogicApp HTTP Rest API\nLogic App SharePoint API\nLogic App OneDrive API\nAPI доступ для получения файла данных API доступ для получения файла данных\nИспользование SharePoint API connector для извлечения файла\nИспользование OneDrive API connector для извлечения файла\nРучное внесение файла через портал\n           Файловые хранилища SharePoint/OneDrive\n       Корпоративный портал SharePoint/OneDrive\n       Прямая загрузка в ALDS\n   Принципы подключения\nВсегда производится подключение Production источника. Источник подключается сначала в Development среду, а при выпуске продукта переносится в продуктивную среду. Параллельно происходит перенос данных в Production среду.\nДля первоначальной загрузки данных из SQL источников применяется следующая процедура:\n1. Фиксирование даты начала интеграции\n2. Загрузка дампа SQL источника на мощности Вендора\n3. Полная загрузка данных до указанной даты интеграции\n4. Включение механизма частичной загрузки после даты начала интеграции\nДанная процедура позволяет исключить чрезмерную загрузку продуктивной системы источника при первой инициализации хранилища Datalake Storage.\nДля файловых источников следует учитывать загрузку каналов передачи данных и временных интервалов возможного их использования.\nРекомендуемые модели загрузки источников данных\nSQL источники\nДля загрузки из SQL источника рекомендуется использовать следующие модели загрузки данных:\n• FULL • DELTA\nFULL модель загрузки данных\nМодель предполагает полную загрузку данных из источника с сохранением информации на каждый день загрузки данных.\nСхема принципа работы\n\n DELTA модель загрузки данных\nМодель предполагает частичную загрузку данных из системы источника за указанный интервал времени. Для реализации данного алгоритма используется подход с фиксированием даты – времени последней загрузки. Для реализации данного механизма необходимо чтобы со стороны системы источника присутствовал признак последнего обновления строки в таблице. Извлеченный объем данных сохраняется на день загрузки.\nСхема принципа работы\nФайловые источники\nДля загрузки файлов данных рекомендуется использовать Azure Data Factory с действием Copy. В случае невозможности извлечения данных с помощью Azure Data Factory следует руководствоваться следующей таблицей:\n    Тип источника FTP/SFTP\nServer File Folder\nHTTP/HTTPS\nВид расположения\nПубличные сети Azure Virtual Machine Закрытые сети\nПубличные сети Azure Virtual Machine Закрытые сети\nLogic App V(1)\nAzure Data Factory\nV V V\nV V V\nМеханизм извлечения\n                                          Публичные сети\nV\n    \n            HTTP Rest API\nSharePoint\nAzure Virtual Machine Закрытые сети\nПубличные сети Azure Virtual Machine Закрытые сети\nВнутренняя сеть\nV(2) V V\nV(3) V V(4) V V\nV(5)\n                            (1) –Logicappприменяетсяеслитребуютсяспецифическиенастройкиподключения.\n(2) –применяетсяеслитребуетсямногошаговыйалгоритм.Например:сначалаполучить\nссылку а потом произвести скачивание файла\n(3) –применяетсяесливозвращаемыйнаборданныхнесоответствуетструктуреDataset\nAzure Data Factory\n(4) –применяетсяесливозвращаемыйнаборданныхнесоответствуетструктуреDataset\nAzure Data Factory\n(5) –применяетсядляизвлеченияфайловизSharePoint.Рекомендуетсяиспользование\nтриггера на изменение или добавление файла.\nРекомендуемая архитектура организации элементов Datalake\nLogic App\nРекомендуется использовать Logic App только для трансфера данных из внешних источников, в случае если данную операцию невозможно осуществить с помощью Azure Data Factory. Ограничение на передачу составляет 100Мб для сервисов, не поддерживающих Chunking Content Transfer. В случае если передаются файлы объемом более 100Мб и не поддерживается Chunking Content Transfer использование Logic App запрещено. При формировании Logic App следует придерживаться следующих правил:\n• Использовать Logic App только для передачи исходных данных в виде «Как есть»\n• Не рекомендуется делать слишком частый триггер на проверку источника.\nРекомендуемый интервал 20 мин.\nПравила именования\ncopy_{source type}_{source name}_{source dataset}_to_{target type}_devla где:\n{source type} – тип источника данных\n  Префикс SP\nOD\nFTP HTTP\nОписание\nСайт в SharePoint портале\nПапка на корпоративном OneDrive Источник FTP/SFTP\nИсточник данных получаемый через HTTP/HTTPS запрос\n          {source name} – наименование источника данных\n{source dataset} – наименование набора данных источника {target type} – тип приемника данных\n\nПример: copy_sp_mireadatalakenaanalytics_testdatastructure_to_adls_devla\nDatalake Storage\nИзвлекаемые данных из источников сохраняются на ресурсе Azure Datalake Storage. Которое предоставляет собой распределенное файловое хранилище в облачном ресурсе.\nСуществуют 3 верхних уровня папок, используемых для работы с данными:\n  Наименование RAW\nPROCESS\nOUTPUT\nОписание\nСодержит «сырые» извлеченные данные\nСодержит промежуточные данные обработки: консолидированные, сопоставленные, трансформированные и т. д.\nСодержит сформированные модели и отчеты\n        Структура элементов\nСтруктура элементов хранилища представляет собой вложенную структуру папок и файлов находящихся в них\n  Путь RAW\nFILES MIREA\n{SYSTEM} {ENVIRONMENT}\n{YYYY} {MM}\n{DD} {ENTITYNAME}\n{METHOD} {EntityName}.csv\n#RECYCLE\n#MAINTENANCE\nSHAREPOINT {SITE NAME}\n{YYYY} {MM}\n{DD}\n{SP Path}\n{FileName}\nFILES INTERNAL\n{SOURCE NAME} {YYYY}\n{MM} {DD}\nОписание\nНаименование системы источника\nНаименование среды (PROD, TEST и т.д.)\nГод\nМесяц\nДень\nНаименование сущности\nМетод получения { FULL | DELTA }\nНаименование файла\nКаталог корзины. Повторяет структуру начиная от {ENVIRONMENT}\nКаталог для временных файлов, необходимых при обработке\nТип источника SharePoint\nНаименование сайта SharePoint\nГод. [Опционально]\nМесяц. [Опционально]\nДень. [Опционально]\nПуть до элемента в SP включая коллекцию Наименование файла\nКорневой каталог для файлов Внутренние файлы компании Наименование источника Год\nМесяц\nДень\nНаименование файла\nФайлы, полученные из внешних источников Наименование источника\n                                                        {FileName} {SOURCE NAME}\n  EXTERNAL\n    \n  {YYYY} {MM}\n{DD} {FileName}\n#DEVELOPMENT {BRANCH}\n{SYSTEM} SHAREPOINT FILES\nPROCESS MIREA\n{SYSTEM} #MAINTENANCE\n#DEVELOPMENT {BRANCH}\n{SYSTEM}\nOUTPUT MIREA\n{MODEL NAME} #ARCHIVE\nMASTERDATA\nMODEL MASTERDATA MODEL QUALITY\n#RECYCLE\n{YYYY} {MM}\n{DD} MODEL\nMASTERDATA\nГод\nМесяц\nДень\nНаименование файла\nКаталог разработки\nНаименование ветки\nПовторяет структуру верхнего уровня с необходимыми данными\nНаименование системы источника\nКаталог для временных файлов, необходимых при обработке\nКаталог разработки\nНаименование ветки\nПовторяет структуру верхнего уровня с необходимыми данными\nНаименование модели\nПредыдущий срез модели до ее обновления Мастер данные для модели\nСама модель\nМастер данные для модели\nСама модель\nОтчеты по качеству, сопоставлению данных. Опционально.\nКорзина для предыдущих срезов данных. Опционально.\nГод\nМесяц\nДень\nКаталог, содержащий данные в процессе разработки Наименование ветки\n                                                                  #DEVELOPMENT {BRANCH}\n      где:\n{SYSTEM} - наименование системы источника данных {METHOD} - тип извлеченных данных FULL | DELTA {ENTITYNAME} - наименование сущности\n{YYYY} - год получения данных из системы источника\n\n{MM} - месяц получения данных из системы источника\n{DD} - день получения данных из системы источника\n{FileName} - наименование файла содержащего данные\n{MODEL NAME} - наименование модели данных построенной на основании сырых данных\n{ENVIRONMENT} – среда источника данных. Показывает откуда были получены данные, продуктивная или тестовая среда\nMASTERDATA - мастер данные необходимые для расшифровки модели\nMODEL – каталог содержащий саму модель данных\nQUALITY – каталог содержащий информацию о качестве данных. (По факту витрины)\n#RECYCLE – каталог, содержащий данные для удаления по истечении определенного времени. Сюда переносятся данные, которые уже устарели либо предыдущие версии разработок. Интервал хранения устанавливается политикой Expire Date Azure Datalake Storage. Например: кардинально изменилась структура модели и старую модель планируется удалить.\n#ARCHIVE – каталог содержит версию модели до обновления. (по факту Backup предыдущей версии) По умолчанию Retention период 1 год.\n#DEVELOPMENT – в данном каталоге находятся подкаталоги с ветками разработок.\n{BRANCH} – наименование ветки разработки (например: BUG-261, CI-4581) Нижележащая структура повторяет структуру каталогов начиная с уровня корневого каталога, исключая папки RECYCLE и ARCHIVE. В данном каталоге содержится структуры данных, относящихся только к конкретной разработке. Тестовый набор данных должен быть достаточен для выполнения доработки. Запрещается копирование полного исторического набора данных.\nДля уровня RAW помещаются данные из источников, которые находятся в состоянии настройки и проверки передачи данных на сторону Datalake. Для Development среды также помещаются данные для UAT (приемо-сдаточного тестирования). В продуктивной среде содержаться только продуктивные данные.\nУровень PROCESS – сюда помещаются данные являющиеся промежуточными при формировании модели и находящиеся\nМодель необходимо располагать в каталоге OUTPUT. Рекомендуется придерживаться следующей структуры каталогов:\nНаименование модели {MODEL NAME}\nКаталог мастер данных {MASTERDATA}\nКаталог самой модели (MODEL)\nКаталог качества сопоставляемых данных {QUALITY}\nВ каталоге мастер данных {MASTERDATA} располагаются сущности справочников для основной модели. В каталоге {MODEL} располагаются сущности фактов модели. Каталог {QUALITY} предназначен для хранения отчетов о качестве данных модели. В него помещаются витрины с не сопоставленными данными, а также витрины отображающие контрольные агрегации, показывающие качество модели.\n\nДля источников Sharepoint структура YYYY/MM/DD является не обязательной, в случае если подобная структура поддерживается в коллекциях документов Sharepoint. Использование структуры YYYY/MM/DD является обязательной если файлы в коллекциях документов обновляются в течении времени.\nДля вариантов «попробовать» алгоритмы обработки данных следует пользоваться каталогом INNOVATION. Данное правило применяется для определения наилучшего алгоритма обработки данных.\nПравила именования\n• Наименования должны содержать только английские буквы.\n• Наименования папок должны быть указаны в верхнем регистре.\n• Все пробелы должны быть заменены на знак «_»\n• Наименование файла для данных извлеченных из SQL систем должны соответствовать\nнаименованию таблицы.\n• Наименование фала полученного из SharePoint или другого источника должно\nсохранять свое наименование. Пример размещения в ADLS:\nКорневой путь RAW/FILES/MIREA\nПример расположения данных находящихся в разработке\nRAW/FILES/MIREA/#DEVELOPMENT/CI- MIREASYSTEM/MIREASYSTEM/2020/02/28/dbo/Table/Table.csv\nПример расположения данных находящихся в RELEASE|PRODUCTIVE RAW/FILES/MIREA/MIREASYSTEM/2020/02/28/dbo/Table/Table.csv\nData Factory\nAzure Data Factory (ADF) является диспетчером управления потоками данных и способами их обработки. С использованием ADF производятся следующие действия:\n• Извлечение\n• Трансформация\n• Вызов внешних ресурсов по отношению к ADF\nДля каждой системы источника должен существовать DISPATCHER pipeline занимающийся координацией работы вложенных pipeline. Запуск данного pipeline должен производится по расписанию или внешнему триггеру.\nДля каждого источника создается отдельный каталог для расположения элементов Azure Data Factory. В зависимости от источника и способа его обработки некоторые папки могут отсутствовать.\nРазработка Pipeline ведется с привязкой конкретной к ветке Git репозитория. После завершения разработки производится Pull Request на объединение разработки с мастер веткой.\n\nСтруктура расположения элементов внутри Azure Data Factory\n  Путь\nPipelines\n{SYSTEM} TEMPLATES MAINTENANCE\nDatasets\n{SYSTEM} TEMPLATES MAINTENANCE\nData flows\n{SYSTEM} Где:\nОписание\nНаименование источника\nСодержит pipeline шаблоны для типовой загрузки данных Содержит pipeline для работы с DevOps процессами\nНаименование источника\nСодержит dataset для типового шаблона\nСодержит dataset для работы с DevOps процессами Не рекомендуется к использованию Наименование источника\n                      {SYSTEM} – наименование источника данных\nДля уровня Pipeline структура папок уровня ниже повторяет структуру папок верхнего уровня. Сюда помещаются pipeline находящиеся в процессе разработки.\nДля уровня Dataset структура папок уровня ниже повторяет структуру папок верхнего уровня. Сюда помещаются dataset относящиеся к процессу разработки.\nДля уровня Dataflows структура папок уровня ниже повторяет структуру папок верхнего уровня. Сюда помещаются Dataflows относящиеся к процессу разработки.\nПравила именования\nPipeline\nШаблон наименования: {SYSTEM}_[ACTION]_{TASK}_PIPE где:\n{SYSTEM} - наименование источника данных\n[ACTION] – наименование стандартного действия, к которому относится pipeline (QC, MAP, VALID).\n{TASK} – наименование действия. Отражает смысловую нагрузку работы pipeline Пример:\nSWE_MAP_OUTLET_PIPE – расположение в папке SWE/MAPPING ARES_CONSOLIDATION_PIPE – расположение в папке ARES SWE_INGESTION_DISPATHCER_PIPE – расположение в папке SWE\nDataset\nШаблон наименования: {SYSTEM}_{DATATYPE}_{ENTITYNANE}_{CONNECTORTYPE}_[FILEFORMAT]_DS где:\n{SYSTEM} - наименование источника данных\n{DATATYPE} – тип содержащихся данных. Не обязательно, если Dataset является параметризированным и используется для различных блоков обработки данных.\n\n  Префикс RAW PRC\nOUT\nОписание RAW data Processed data Consumer data\n        {ENTITYNANE} – наименование сущности. Не обязательно, если Dataset является параметризированным и используется для различных блоков обработки данных.\n{CONNECTORTYPE} - наименование типа набора\n[FILEFORMAT] – суффикс формата файла. Применятся для случая если один набор может сохраняться в различных форматах. Допустимые суффиксы типа формата:\nСуффикс Наименование PRQ Parquet\nAVR Avro\nCSV Comma delimited JSN JSON\nORC Orc BIN Binary\nПример:\nMIREASYSTEM_RAW_TABLES_ADLS_DS или SWE_TABLES_ADLS_CSV_DS\nLinked Service\n{SYSTEM}_{DATASOURCE}_{CONNECTORTYPE}_LS где:\n{SYSTEM} - наименование источника данных\n{DATASOURCE} – источник данных\n{CONNECTORTYPE} – тип подключения Пример:\nMIREASYSTEM_MIREADB01_MIREASYSTEMPROD_SQLSRV_LS\nInegration Runtime\n{LOCATION}-{ENV}-IR\nгде:\n{LOCATION} – расположение на уровне системы или территориальное расположение.\n{ENV} – среда подключения Пример:\nRESOURCEGROUPMIREA-MIREASYSTEM-PROD-IR\nTrigger\n{PIPELINE}_{SCHEDULETYPE}_TRG где:\n                \n{PIPELINE} – наименование pipeline для вызова\n{SCHEDULETYPE} – тип периодичности вызова триггера Пример:\nMIREASYSTEM_INGESTION_DISPATCHER_DAILY_TRG MIREASYSTEM_INGESTION_DISPATCHER_EVENT_TRG\nDatabricks\nВсе блокноты должны располагаться в папках с наименованием источника, к которому они относятся.\nСтруктура расположения элементов\n  Путь RELEASE\n{SYSTEM} SHARED\nDEVELOPMENT {SYSTEM}\nSHARED\nBRANCH [CI/FIX]_DESC\nОписание\nПапка содержит RELEASE Наименование системы Каталог общих блокнотов\nИнтеграционный каталог Наименование системы Каталог общих блокнотов\nОбщий каталог правки/разработки\nКаталог правки/разработки по конкретному CI/FIX Наименование системы\nКаталог общих блокнотов\n                      {SYSTEM} SHARED\n      где:\nRELEASE – содержит блокноты находящиеся в UAT процессе и готовые в любой момент для вывода в продуктив.\nBRANCH – содержит блокноты находящиеся в процессе разработки.\nDEVELOPMENT - интеграционный каталог, содержит блокноты при слиянии веток. Структура каталогов повторяет структуру верхнего уровня\n[CI/FIX]_DESC – папка содержащая блокноты для выполнения работ по разработке / правке. Блокноты в этой ветке привязаны к файлам репозитория.\nПравила именования\n1. Наименования блокнотов должны быть в верхнем регистре с использованием букв английского алфавита.\n2. Наименование блокнота должно отражать выполняемую задачу\nДля блокнотов, обрабатывающих данные конкретного источника шаблон наименования:\n{SYSTEM} – наименование системы источника\n\n{SYSTEM}_{TASK}_{TYPE}\n{SYSTEM} – наименование системы источника\n{TASK} – смысловое определение выполняемой блокнотом задачи {TYPE} – тип задачи. Может отсутствовать\nПример: ARES_ROWSCOUNT_QC\nSWE_DBSCHEMA_VALIDATION\nДля блокнотов формирующих результирующие модели данных шаблон наименования: MODEL_{MODELNAME}_[ENTITYNAME]_[MD]\n{MODELNAME} – наименование результирующей модели\n[ENTITYNAME] – наименование формируемой сущности. Может отсутствовать в случае если блокнот формирует модель целиком.\n[MD] – признак что формируются мастер данные модели Пример:\nMODEL_MIREASYSTEM_ORGSTRUCT_MD\nMODEL_MIREASYSTEM_KPIFACT\nПример расположения блокнотов с учетом веток:\nWorkspace/BRANCH/CI-MIREASYSTEM/SHARED/MONITORING_HELPER\nWorkspace/BRANCH/CI-MIREASYSTEM/MIREASYSTEM/MIREASYSTEM_BUILD_MODEL Пример расположения блокнотов с учетом веток:\nWorkspace/RELEASE/MIREASYSTEM/MIREASYSTEM_BUILD_MODEL\nЛитература\n1. Сенько А., Работа с BigData в облаках. Обработка и хранение данных с примерами из Microsoft Azure, Издательство «Питер», 2019\n2. Richard Nuckolls. Azure Data Engineering. Manning Publications. 2020\n3. Майк Уоссон, Макаси Нарумото. Руководство по архитектуре облачных\nприложений. Microsoft Press, 2017\n4. Майкл Коллиер, Робин Шаан. Основы Microsoft Azure. Microsoft Press, 2017\n5. Jeffrey Palermo, .NET DevOps for Azure: A Developer’s Guide to DevOps Architecture\nthe Right Way. APress, 2019\n6. Иоахим Хафнер, Саймон Швингель, Тайлер Айерс и Рольф МакЛохлин,\nРуководство по стратегии и реализации Azure. Корпорация Microsoft, 2018\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 8.\nШаблоны ETL процессов AZURE DATALAKE для различных типов источников данных\nОсновной ETL процесс обработки информации:\n1. Извлечение данных из источника\n2. Проверка качества загрузки данных из источника\n3. Проверка структуры данных источника\n4. Консолидирование данных (по необходимости)\n5. Формирование модели\nЛюбой элемент процесса ETL должен быть подвержен мониторингу технических сбоев. Это означает что можно получить информацию о работе элемента на указанном интервале времени с использованием внешних средств доступа к Datalake. Например: PowerShell\nВсе функциональные сбои должны отображаться в специализированных витринах с последующим выводом данных в отчеты или средства визуализации (Power Bi, Tableu и.т.д)\nВ проекте крайне желательно реализовать возможность формирования процессов ETL из шаблонных решений. Все шаблоны имеют общую базовую структуру с небольшими вариациями в зависимости от типа источника данных. Описание шаблонов находится в документе «Описание шаблонов ETL по типам источников данных»\nОбщая схема извлечения и обработки данных из источника:\n Необходимые структурные элементы по организации процесса загрузки и обработки данных\n• Схема источника данных\n• Схема консолидированных данных\n• Схема модели данных\n• Мониторинговые таблицы для каждого из уровней (RAW, PROCESS, OUTPUT)\n• Таблицы проверки качества для каждого из уровней (RAW, PROCESS, OUTPUT)\n• Таблица настроек\nДля каждого источника всегда существует Pipeline диспетчер (DISPATCHER). Данный Pipeline вызывается по триггеру или внешнему событию и управляет отдельными pipeline по извлечению данных, обработке (консолидировании) данных, построения модели.\nАлгоритм работы pipeline извлечения данных\n1. Получение настроек путей сохранения загруженных данных.\n\n2. Получение схемы источника.\n3. Внесении информации В БД о начале получения данных.\n4. Получения данных согласно полученной схеме.\n5. Внесении информации В БД об окончании получения данных.\n6. Внесении информации В БД о начале проверки качества данных.\n7. Проверка качества данных.\n8. Внесении информации В БД об окончании проверки качества данных.\nАлгоритм работы pipeline консолидирования/обработки данных\n1. Получение схемы консолидирования.\n2. Получение схемы источника.\n3. Внесении информации В БД о начале проверки качества данных.\n4. Проверка возможности обработки загруженных данных.\n5. Внесении информации В БД об окончании проверки качества данных.\n6. Внесении информации В БД о начале консолидирования/обработки данных.\n7. Консолидирование/обработка полученных данных. Применяется жизненный цикл\nданных.\n8. Внесении информации В БД об окончании консолидирования/обработки данных.\nАлгоритм работы pipeline формирования модели данных\n1. Внесении информации В БД о начале формирования модели данных.\n2. Формирование модели данных.\n3. Внесении информации В БД об окончании формирования модели данных.\n4. Внесении информации В БД о начале проверки качества модели данных.\n5. Проверка качества модели данных. Применяется жизненный цикл данных.\n6. Внесении информации В БД об окончании проверки качества модели данных.\nЖизненный цикл данных в Azure Datalake Storage\nВ областях PROCESS и OUTPUT данные проходят определенный цикл жизни. Схема жизненного цикла:\nВ каталоге #RECYCLE данные хранятся согласно политики хранения исторических версий. Очистка производится со стоhоны Datalake путем установки Expire Date либо Maintenance процедурой, предусмотренной для источника.\nШаблоны Logic App для различных типов ADLS\nВ проекте предусмотрена возможность создания Logic Apps копирования данных из шаблонных решений. Все шаблоны имеют общую базовую структуру с небольшими вариациями в зависимости от типа источника данных и типа хранилища (Gen1/Gen2).\n \nАлгоритм работы Logic App копирования данных в ADLS Gen1\n1. Срабатывание триггера на создание/изменение данных\n2. Получение настроек для текущего Logic App из БД\n3. Формирование пути для копирования файла\n4. Получение данных из источника\n5. Загрузка данных в хранилище\n6. Внесении информации В БД о загрузке файла\nАлгоритм работы Logic App копирования данных в ADLS Gen2\n1 Срабатывание триггера на создание/изменение данных\n2. Получение настроек для текущего Logic App из БД\n3. Формирование пути для копирования файла\n4. Получение данных из источника\n5. Получение токена для доступа к API ADLS Gen2\n6. Загрузка данных в хранилище\n7. Внесении информации В БД о загрузке файла\nТребования к документации загрузки данных\nДля каждого источника данных при формировании документации рекомендуется заполнить следующие пункты:\n• Описание источника\n• Тип источника\n• Принципы подключения\n• Модель\n• Параметры\nОписание источника\nВ данном пункте приводится информация о источнике данных. Рекомендуется заполнить следующие пункты:\n• Наименование источника – наименование источника\n• Владелец источника – компания производитель системы\n• Предметная область источника – описание предметной области, к которой относится\nисточник данных.\n• Время доступности - временные интервалы доступности системы источника для\nизвлечения данных\n• Частота извлечения – частота извлечения данных из источника с учетом временной\nдоступности\n\n• Критичность – критичность источника для формирования витрин данных Пример:\n• Наименование системы: SalesWorks Enterprise\n• Предметная область: Система автоматизация работы торговых агентов и полевых\nсотрудников\n• Владелец системы: SoftServe Business Systems\nТип источника\nВ данном пункте описываются все типы источников используемых при загрузке данных. Для SQL баз данных следует указать тип базы данных, для файловых источников следует указать тип загружаемого файла.\nПример: (SharePoint источник) Тип источника: SharePoint 2013 Тип предоставляемых данных:\nФайлы справочников предоставляются в формате xlsx. Файлы фактов предоставляются в формате csv.\nФайлы изображений предоставляются в формате jpeg\nПринципы подключения\nВ данном разделе указываются принципы подключения к источнику данных. Должна быть указана следующая информация:\n• Адрес источника. IP адрес / Доменное имя\n• Логин – Пароль для доступа\n• Механизм извлечения данных, Integration Runtime, logic App, Web Request\nМодель\nВ данном разделе рекомендуется заполнить следующие пункты:\n• Структурная схема модели. Общая, связей по ключевым полям, полная.\n• Данные о маппинге полей модели с источниками\n• Алгоритмы формирования модели\nСтруктурная схема модели\nВ данном пункте рекомендуется привести схему взаимодействия сущностей модели с указанием типа сущности, факты или мастер данные. Рекомендуется привести 3 вид схем: общую схему взаимосвязей сущностей с указанием только самих сущностей без указания полей, схему взаимосвязей по ключевым полям с указанием сущностей и ключевых полей, полную схему модели с полным перечнем полей и взаимосвязями между сущностями.\nВ дальнейшем предполагается автоматическое формирование схем на основе технических таблиц описания данных с добавлением специального описателя связей между элементами внетри каждого уровня. Уровни описания рекомендуется разделять на следующие типы:\n• Источник (RAW)\n• Консолидация (PROCESS)\n• Модель (OUTPUT)\n  \nПредставление схем является логической структурой и не обязательно соответствует расположению файлов.\nРекомендуемые принципы формирования моделей\nРекомендации к гранулярности\nРекомендуется использовать минимальную гранулярность для формирования моделей. Гранулярность модели определяется минимальной гранулярностью данных входящих в эту модель.\nРекомендации по документированию модели\nПри составлении документации по модели рекомендуется включить следующие пункты\n• Цели формирования и предназначение модели\n• Для масштабных моделей опционально предлагается дробить схему описания в виде 3-\nх уровневой схемы – 1й уровень, это схема связей между сущностями модели. 2й уровень, это схема связей с указанием полей связей. 3й уровень, это схема с указание полного перечная полей сущности модели.\n• Описание маппинга полей на источники данных – в данном пункте приводятся сопоставление полей модели с полями источников, поставляющих данные для сущности модели.\n• Описание структуры хранения - в данном пункте следует разместить описание структуры каталогов расположения сущностей модели. Так же рекомендуется указать краткое описание.\n• Описание Pipeline – в данном пункте следует указать наименования Pipeline участвующих в формировании модели. Для каждого Pipeline следует указать перечень параметров и их описание, в случае если pipeline является параметризированным. В качестве описания допускается приведение снимков экрана со структурой Pipeline\n• Описание блокнотов – в данном пункте приводится перечень блокнотов Databricks с кратким описанием алгоритма работы. Если блокнот является параметризированным, то следует указать наименование параметров и их описание.\nЛитература\n1. Сенько А., Работа с BigData в облаках. Обработка и хранение данных с примерами из Microsoft Azure, Издательство «Питер», 2019\n2. Richard Nuckolls. Azure Data Engineering. Manning Publications. 2020\n3. Майк Уоссон, Макаси Нарумото. Руководство по архитектуре облачных\nприложений. Microsoft Press, 2017\n4. Майкл Коллиер, Робин Шаан. Основы Microsoft Azure. Microsoft Press, 2017\n5. Jeffrey Palermo, .NET DevOps for Azure: A Developer’s Guide to DevOps Architecture\nthe Right Way. APress, 2019\n6. Иоахим Хафнер, Саймон Швингель, Тайлер Айерс и Рольф МакЛохлин,\nРуководство по стратегии и реализации Azure. Корпорация Microsoft, 2018\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 9.\nПроцесс разработки начинается с создания пустой системы, системы из шаблона или выделения новой ветки для существующей системы.\nРазработка в Azure Data Factory производится только в ветках CI, FIX, TEMPLATE. Обычно запрещается вносить какие-либо изменения напрямую в интеграционные ветки development и master. После окончания разработки производится слияние ветки разработки с интеграционной веткой.\nРазработка в Azure Data Lake Store ведётся только в папках #DEVELOPMENT. После окончания разработки производится перенос элементов в структуру папок верхнего уровня.\nРазработка в Azure Databricks ведётся только в папке BRANCH. Запрещается вносить изменения в каталогах DEVELOPMENT, RELEASE. После окончания разработки производится автоматический перенос блокнотов в каталог RELEASE с затиранием всех данных в каталоге и последующем выводе всех блокнотов из ветки.\nРазработка в Azure Database ведётся только в схемах с префиксом Dev_:\nПосле релиза в UAT среду и тестирования запускается процедура переноса разработки в продуктивную среду.\n \nСохранение структур данных Azure Database в ветку разработки\nПри создание новой ветки разработки на все таблицы и хранимые процедуры установлен триггер который автоматически сохраняет все изменения вносимые в хранимые процедуры, схемы таблиц с префиксом Dev_ в ветку разработки.\nИзменения внесенные в хранимые процедуры сохраняться в ветку разработки в каталог database по пути /database/{SystemName} с именем {StoredProcedureName}.sql.\nИзменения внесенные в схемы таблиц сохраняться в ветку разработки в каталог database по пути /database/{SystemName} с именем {TableName}_SCHEMA.sql.\n \nСохранение настроек в ветку разработки\nВсе настройки с типом Durable хранимые в таблице сохраняются в ветку разработки в виде CSV-файла.\n При внесении изменений в таблицу параметров срабатывает триггер записывающий информацию об имениях в таблицу ParametersChange на записи в данную таблицу происходит событие на срабатывание pipeline Azure DevOps который генерирует CSV с Durable параметрами и коммитит полученный CSV-файл в ветку разработки.\nСоздание пустой системы\nДля создания новой системы необходимо проделать вручную или автоматизировать следующие действия:\n• Новая ветка в системе контроля версий Git с именем указанным в переменной BranchName\n• Пустые pipeline со стандартными имена в Azure Data Factory помещенные в каталог с именем системы\n• Каталог в Azure Databricks с именем системы для разработки блокнотов по следующему пути /BRANCH/{BRANCHNAME}/{SYSTEMNAME}\n• Таблицы в Azure Database\no Таблицы генерации скриптов и параметров в схеме Dev_Setting o Таблицы мониторинга в схеме Dev_Monitoring\no Таблицы QC в схеме Dev_QC\n• Структура каталогов в Azure Data Lake Store\n\no RAW/FILES/#DEVELOPMENT/{BRANCHNAME}/{SYSTEMNAME} o PROCESS/#DEVELOPMENT/{BRANCHNAME}/{SYSTEMNAME} o OUTPUT/#DEVELOPMENT/{BRANCHNAME}/{SYSTEMNAME}\nШаблоны разработки\nШаблонами разработки являются наборы Pipeline & notebook располагающихся в папке TEMPLATES.\nОбычно разрабатываются шаблоны доставки данных в зависимости от типа источника, например, MSSQL, ORACLE, POSTGRESQL, SFTP и т.д.\nТакже нередко существует дополнительное выделение в качестве шаблонов алгоритмы и механизмы извлечения данных, например, различные алгоритмы извлечения данных в виде Delta по версии записи, алгоритмы извлечения по фиксации изменения в дополнительных таблицах, изменения данных связанные с датой модификации файлов и другие.\nТакие шаблоны могут быть использованы для формирования pipeline и скриптов загрузки новой системы на основе шаблонизированного и параметризированного шаблона.\nПри этом важно реализовать механизмы обновления уже созданных элементов при изменении шаблонов.\nТакие механизмы могут быть реализованы через стандартный механизм ARM TEMPLATE в Azure Data Factory, или с помощью механизмов автоматизации AZURE DevOps с потоковой трансформацией структур всех JSON представлений pipeline в ветке и возвратом изменений.\nБолее сложный вариант используется в том случае, если в рамках шаблона используются не только элементы ADF, но и другие, связанные структуры, например, скрипты Azure Data Bricks.\n\nСоздание Pull Request\nДля создания Pull Request необходимо в Azure Data Factory перейти в ветку, в которой велась разработка и нажать кнопку Publish. После нажатия будет выведено диалоговое с предложениеv создать Pell Reuqest, со следующим содержимым «Publish is only allowed from collaboration ('development') branch. Merge the changes to 'development'.» Необходимо перейти по ссылке «Merge the changes to 'development'»\n Проверяем ветку из который будут выводиться изменения и интеграционную ветку. После проверок нажимаем Create.\n На следующем этапе проверяем возникшие конфликты, решаем их и по завершению нажимаем Complete.\n\nВ появившемся диалоговом окне «Complete pull request» убираем чекбокс «Delete {SOURCE_BRANCHNAME} after merging» и нажимаем Complete merge.\nВетка разработки была влита в интеграционную ветку.\nПодключение блокнотов Azure Databricks к системе контроля версий\nДля сохранений изменений и версионирования блокнотов Azure Databricks разработка должна вестись только с использованием системы контроля версий Git. При создании новой ветки все блокноты автоматически линкуются к репозиторию Git, но при создании нового блокнота понадобиться добавить его в Git вручную (см. п. Подключение нового блокнота).\nПодключение нового блокнота\nПосле создание нового блокнота его необходимо подключить Git репозиторию, для этого необходимо выбрать панель Revision History, нажать на ссылку Git: not linked, после чего откроется диалоговое окно Git Preferences. В открывшемся окне необходимо изменить статус блокнота на Link, указать ссылку до репозитория (https://marsanalytics@dev.azure.com/marsanalytics/RUSSIA%20DATA%20FOUNDATION/_git/russ ia-data-foundation) выбрать необходимую ветку и изменить путь блокнота согласно следующему шаблону:\n1. В начале пути добавить каталог data-bricks\n2. Удалить каталог BRANCH\n3. Удалить имя ветки\n \nПример:\nБыло: notebooks/BRANCH/ci-hydrateatlas/APOLLODEMAND/APOLLODEMAND_QC.py\nНеобходимо: data-bricks/notebooks/APOLLODEMAND/APOLLODEMAND_QC.py\n  \nРегламент написания кода в Azure Databricks\nРегламент написания кода приводится в документе “Регламент написания Python кода в Azure Databricks”\nВывод разработки в UAT\nПосле завершение разработки в своей ветке необходимо вывести все изменения в интеграционную ветку development. Для этого в своей ветке разработки CI необходимо создать Pull Request (см. п. Создание Pull Request) на вывод всех имений в интеграционную ветку.\nОтветственный за UAT разработчик должен принять Pull Request или отказать в приеме с обоснованием и указанием недостатков, которые требуется устранить до подачи повторного запроса. Для проверки корректности запроса на слияние необходимо выполнить обязательную процедуру проверки всех внесённых изменений в Azure Data Factory с помощью кнопки Validate All для каждого изменения. Для изменений Azure Databricks и изменений других элементов выполняется CodeReview.\nВ случае принятия запроса ответственный должен произвести публикацию всех изменений для UAT (нажать кнопку Publish для публикации изменений в Data Factory и запуска Azure DevOps pipeline с выводом изменений остальных элементов).\n При публикации изменений в Data Factory будет сгенерирован ARM шаблон который будет помещен в ветку adf_publish. После генерации, автоматически по срабатыванию триггера, будет запущен Azure DevOps pipeline который выведет из интеграционной ветки следующие изменения:\n• Блокноты Azure Databricks\n• Приложение логики Logic App\n• Синхронизирует параметры\n\n• Синхронизирует схемы таблиц Azure Database\n• Синхронизирует хранимые процедуры Azure Database для генерации скриптов\nВывод разработки в Production\nПо завершению тестирования в UAT среде необходимо провести слияние интеграционной ветки development разработки с интеграционной веткой продуктива master.\nОтветственный за продуктив разработчик должен принять Pull Request или отказать в приеме с обоснованием и указанием недостатков, которые требуется устранить до подачи повторного запроса. Для проверки корректности запроса на слияние необходимо выполнить обязательную процедуру проверки всех внесённых изменений в Azure Data Factory с помощью кнопки Validate All для каждого изменения. Для изменений Azure Databricks и изменений других элементов выполняется CodeReview.\nВ случае принятия запроса ответственный должен произвести публикацию всех изменений для продуктива (нажать кнопку Publish для публикации изменений в Data Factory и запуска Azure DevOps pipeline с выводом изменений остальных элементов).\n При публикации изменений в Data Factory будет сгенерирован ARM шаблон который будет помещен в ветку adf_publish. После генерации, автоматически по срабатыванию триггера,\nбудет запущен изменения:\n• • • • •\nAzure DevOps pipeline который выведет из интеграционной ветки следующие\nБлокноты Azure Databricks\nПриложение логики Logic App\nСинхронизирует параметры\nСинхронизирует схемы таблиц Azure Database\nСинхронизирует хранимые процедуры Azure Database для генерации скриптов\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 10.\nКлассический принцип организации CI/CD для AZURE DATA FACTORY\nВ Azure Data Factory непрерывная интеграция и доставка (CI/CD) означает перемещение конвейеров Data Factory из одной среды (разработка, тестирование, производство) в другую. Для использования CI/CD можно использовать интеграцию Data Factory UX с шаблонами менеджера ресурсов Azure Data Factory (ARM TEMPLATE).\nДля целей развертывания в AZURE используется специальные шаблоны AZURE Resource Manager для каждой среды/\n  В интерфейсе фабрики данных можно сгенерировать такой шаблон с помощью экспорта ARM TEMPLATE, в результате чего будет создан шаблон для открытой в интерфейсе фабрики данных и файл конфигурации, включающий все строки подключения и другие параметры. Затем создаются файлы конфигурации для каждой из сред (разработка, тестирование и т.д.). Базовый файл шаблона остается без изменений и одинаков для всех сред.\nСтандартная схема автоматизации непрерывной интеграции с помощью релизов Azure Pipelines\nНа рисунке приведена стандартная схема Azure Pipelines для автоматизации развертывания ADF в нескольких средах.\n \n Настройка выпусков через AZURE DEVOPS Pipelines\nДля настройки простого механизма развертывания в Azure DevOps необходимо пройти следующую процедуру:\n 1. В AZURE DEVOPS откройте проект, настроенный на фабрику данных.\n 2. На левой стороне страницы выберите Pipelines, а затем выберите Releases.\n3. Выберите New pipeline или, если у вас есть существующие конвейеры,\nвыберите New, а затем New release pipeline.\n4. Выберите пустой шаблон службы.\n \n 5. Внесите название среды развертывания в Stage name.\n6. Добавьте новый артефакт, а затем выберите репозиторий с фабрикой\nданных. Выберите adf_publish для ветки по умолчанию. Для версии по умолчанию - выберите Latest from default branch.\n   7. Добавьте задачу развертывания Azure Resource Manager. а. В представлении этапа выберите View stage tasks.\n  \n b. Создайте новое задание Azure Resource Group Deployment. c. В задаче развертывания выберите подписку, группу ресурсов и\nместоположение для фабрики данных.\nd. В списке действий выберите Create or update resource group.\ne. Выберите ранее экспортированный файл с ARM шаблоном - находится в папке ветки adf_publish.\nf. В Template parameters необходимо выбрать файл ARMTemplateParametersForFactory.json, который может быть стандартным файлом настроек или содержать изменения.\ng. В Override template parameters необходимо внести изменения для фабрики назначения. Для учетных данных, получаемых из Azure Key Vault, имя ключа вводится между двойными кавычками.\nh. Необходимо выбрать Incremental в Deployment mode.\nПри выборе Complete в Deployment mode существующие ресурсы могут быть удалены полностью!\n \n 8. Сохраните pipeline\n9. Запуск развертывания производится выбором Create release.\n   В результате релиза ADF будет развернут на целевую среду.\nПолучение паролей из Azure Key Vault\n1. Добавить секреты в файл параметров.\nЕсли у вас есть пароли для передачи в ARM Template Azure, то для их хранения и развертывания рекомендуется использовать Azure Key.\nСуществует два способа обработки паролей:\n  Создается копия файла параметров, загруженного в ветку\nпубликации. Установливаются значения параметров, которые нужно получить от Key Vault, используя следующий формат:\nJSONКопировать\n  {\n    \"parameters\": {\n        \"azureSqlReportingDbPassword\": {\n            \"reference\": {\n                \"keyVault\": {\n                    \"id\":\n\"/subscriptions/<subId>/resourceGroups/<resourcegroupId> /providers/Microsoft.KeyVault/vaults/<vault-name> \"\n},\n\n } }\n}\n    \"secretName\": \" < secret - name > \"\n}\nПри использовании этого метода пароли будут автоматически извлекаться из хранилища.\nФайл параметров должен находиться в ветви публикации.\n2. Добавить задачу Azure Key Vault перед задачей развертывания ARM:\n 1. В задаче «Azure Key Vault» необходимо выбрать подписку, в которой создано хранилище ключей и внести параметры доступа\n   Использование пользовательских параметров с шаблоном ARM\nЕсли подключен GIT, можно переопределить свойства по умолчанию в шаблоне ARM для установки свойств, которые параметризируются в шаблоне, и свойств, которые\n вностятся жестко(hardcode). В некоторых случая может потребоваться переопределить шаблон параметризации по умолчанию:\n • используется автоматизированный CI/CD и нужно изменить некоторые свойства во время развертывания диспетчера ресурсов, но свойства не параметризируются по умолчанию.\n• В шаблоне больше, чем максимально допустимое количество параметров (256 шт.).\n \nВ этом случае, чтобы переопределить шаблон параметризации по умолчанию, создается файл с названием arm-template-parameters-definition.json в папке, указанной в качестве корневой папки для git. Data Factory считывает этот файл из той ветви, которая выбрана на портале Azure Data Factory. Если файл не найден, используется шаблон по умолчанию.\n Синтаксис пользовательского файла параметров\nФайл параметров состоит из разделов для каждого типа сущности: триггер, конвейер, связанная служба, набор данных, время выполнения интеграции и так далее.\n • путь свойства указывается под соответствующим типом сущности.\n• Установка имени свойства в * используется для указания того, что необходимо\nпараметризировать все свойства под ним (только до первого уровня, а не\nрекурсивно).\n• Установка значения свойства как строки указывает на то, что свойство\nпараметризируется, при этом формат записи следующий: <action>:<name>:<stype> .\n o <action> может быть одним из этих символов:\n ▪ = -означает сохранение текущего значения в качестве значения по умолчанию для параметра.\n▪ - - означает отсутствие значения по умолчанию для параметра.\n▪ | - это специальный знак для строк или ключей соединения из Azure Key\nVault.\n o <name> — это название параметра. Если он пуст, он берет название\nсвойства. Если значение начинается с символа «-», то имя\nсокращается. Например, AzureStorage1_properties_typeProperties_conne ctionString будет сокращен до AzureStorage1_connectionString .\no <stype>-этотиппараметра.Если<stype>пуст,поумолчаниюиспользуется тип string. Поддерживаемые значения типов: string, bool, number, object и securestring .\n • Указание массива в файле определения указывает на то, что соответствующее свойство в шаблоне является массивом. Data Factory итерирует все объекты в массиве, используя определение, указанное в объекте времени выполнения интеграции массива. Второй объект, строка, становится именем свойства, которое используется в качестве имени параметра для каждой итерации.\n• Определение не может быть специфичным для экземпляра ресурсов. Любое\n определение применяется ко всем ресурсам такого типа.\n• По умолчанию параметризируются все объекты безопасности, такие как пароли\nKey Vault, а также строки соединения, ключи и токены.\n  Связанные шаблоны Resource Manager\nПри использовании CI/CD для фабрик данных, существует вероятность превысить пределы шаблона ресурсов ARM из-за роста размера фабрики. Например, одним из ограничений является максимальное количество ресурсов в шаблоне. Для работы с\n \nбольшими фабриками для шаблона фабрика теперь генерирует связанные ARM шаблоны вместо одного большого. С помощью этой функции вся загрузка на фабрику разбита на несколько файлов, чтобы избежать ограничений.\nВ Git, связанные шаблоны генерируются и сохраняются в adf_publish в отдельной папке linkedTemplates:\n   Связанные шаблоны обычно состоят из основного шаблона и набора связанных с ним подчиненных шаблонов. Родительский шаблона называется ArmTemplate_master.json, а подчиненные шаблоны именуются в виде ArmTemplate_0.json, ArmTemplate_1.json и так далее.\nЧтобы использовать связанные шаблоны вместо полного нужно обновить задачу CI/CD, и выбрать ArmTemplate_master.json вместо ArmTemplateForFactory.json. Менеджер ресурсов также требует, хранения связанных шаблонов в Storage account, чтобы Azure мог получить к ним доступ во время развертывания.\nИспользование ветки Hotfix\nИногда в продуктивной среде ADF требуется быстро исправить ошибку, но не возможно вывести интеграционную ветку. Для решения этой проблемы рекомендется создать и использовать для исправления ветку HotFix следующим образом:\n 1. В Azure DevOps найдите релиз, который был развернут в продуктивную\n среду. Выберите последний выпущенный коммит.\n2. Из сообщения о коммите получите идентификатор в интеграционной ветке.\n3. Создайте новую ветку hotfix из этого коммита.\n4. Перейдите на UX-фабрику данных Azure и переключитесь на ветку hotfix.\n5. Используя Фабрику данных Azure UX, исправьте ошибку. Проверьте свои\nизменения.\n6. После проверки исправления выгрузите ARM шаблон hotfix.\n7. Вручную проверьте эту сборку в adf_publish ветке.\n \n8. Если настроен релизный pipeline на изменения adf_publish, то новый релиз запустится автоматически, если нет, то необходимо запустить релиз вручную.\n9. Необходимо протащить изменения из hotfix в ветку разработки, чтобы исключить потерю hotfix изменений .\n ",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 11.\nВнастоящее время Azure предоставляет два сервиса нереляционных хранилищ данных: традиционный Azure Table Storage и Azure CosmosDB. Первый — хранилище типа «ключ — значение». Второй — набор глобально распределенных нереляционных хранилищ, объединенных общей концепцией управления и создания, к которым в настоящее время относятся базы данных DocumentDB и MongoDB (документоориентированные), Gremlin (графовая база данных), Table (таблица типа «ключ — значение») и Cassandra (база данных, относящаяся к типу семейства столбцов).\nНачнем с рассмотрения Azure Table Storage. Этот сервис является встроенным в Azure Storage Account и создается на общей панели, содержащей остальные сервисы (Blob, File и Queue). Общая панель доступа к Azure Table Storage такая же, как и для других сервисов, включенных в Azure Storage. Она очень проста и содержит минимальные опции для конфигурации\nЭто хранилище поддерживает протокол OData для выборки данных на основе протокола HTTP, так что таблица имеет конфигурации для настройки CORS и политики доступа. Сам сервис очень минималистичен, и основное конфигурирование будет происходить не на портале, а с помощью кода\nСамо по себе хранилище Azure Table Storage предназначено для хранения больших объемов (в настоящее время это 500 Тбайт на один Storage Account) структурированных нереляционных данных. Под структурированностью здесь понимается тот факт, что все сущности («строки») представляют собой набор пар «ключ — значение» (причем не обязательно, чтобы он был одинаковым во всех строках). По сути, это огромные таблицы, которые нецелесообразно размещать РБД. В качестве примера можно привести сырые потоковые данные, помещенные таблицу сервисом Stream Analytics Job,\n \nкоторые в итоге должны быть агрегированы сервисом ETL (например, сервисом Azure DataFactory в комбинации с Azure HDInsight Spark) и помещены в реляционное хранилище данных (Azure SQL DataWarehouse) или просто в обычную РБД. При этом желательно сохранить все сырые данные, чтобы в последующем можно было выполнить их интерактивный или интеллектуальный анализ. Azure Table Storage позволяет выполнить такую операцию ввиду того, что способен легко интегрироваться с сервисами копирования, трансформации и потокового анализа данных. Кроме того, поддержка протокола OData для прямого доступа и наличие SDK позволяет получить прямой доступ к табличному хранилищу извне. При этом выполнение высокопроизводительных запросов возможно благодаря встроенному механизму кластерных индексов.\nГлавные компоненты Azure Table Storage\nИтак, Storage Table Account включает в себя набор таблиц, каждая из которых вмещает набор сущностей, то есть строк. Каждая сущность состоит из набора свойств и имеет суммарный размер не более 1 Mбайт. В свою очередь, свойство — это пара «имя — значение». Каждая сущность может иметь до 256 свойств, три из которых обязательны: ключ раздела (partition key), ключ строки (row key) и временная метка (timestamp). Временная метка соответствует времени последней модификации этой сущности. Как уже указывалось, ключ строки должен быть уникальным в пределах раздела, а комбинация «ключ раздела — ключ строки» — уникальной глобально. Для набора, состоящего из сущностей с одинаковым значением ключа раздела, значения могут быть очень быстро выбраны с помощью запроса. Операции вставки/удаления/обновления в данном случае тоже выполняются быстрее. Это возможно потому, что на оба ключа создаются кластерные индексы. Никакие другие индексы создать нельзя.\nТипы данных, которые поддерживаются Table Storage, полностью соответствуют типам данных, доступных в протоколе OData.\nТипы данных стандарта OData\nТип данных CLRтип,\nOData доступный Примечание\n           \n    в коде\n      Edm.Binary Edm.Boolean\nEdm.DateTime\nEdm.Double\nEdm.Guid Edm.Int32 Edm.Int64\nEdm.String\nbyte[ ] Bool\nDateTime\nDouble\nGuid\nint / Int32 long / Int64\nString\nМассив байтов размером до 64 Кбайт\nБулево значение\n64разрядное значение, представляющее собой временную метку UTC\n64разрядное число с плавающей точкой\n128разрядный глобальный идентификатор\n32разрядное целое 64разрядное целое\nСтроковая величина с кодировкой UTF16.\nМаксимальный размер строки — 64 Кбайт\n                                                      Тип по умолчанию — строковый. Для хранения более сложные типы данных должны быть сериализованы в XML или JSON и помещены в строковый формат. Второй вариант — сериализация в двоичный формат и помещение его в битовый массив.\nМодели хранения данных нереляционных хранилищ типа «ключ — значение» не ограничиваются структурами типа «одна большая таблица» или «набор несвязанных таблиц». Можно моделировать отношения и построение таблиц, оптимизированных для решения определенного круга задач (быстрые запросы на выборку, быстрая модификация и пр.).\nРассмотрим теперь базы данных от сервиса Azure CosmosDB. Он является относительно новым и предоставляет единую программную модель для доступа к нереляционным базам разных типов:\n• DocumentDB, она же SQL API, — документоориентированная база данных возможностью выполнения запросов с помощью как SQL, так и JavaScript;\n• MongoDB — облачный сервис хорошо известной базы с таким же названием;\n• Graph API — сервис графовой базы данных, называемой еще Gremlin;\n\n• Table API — дальнейшее развитие базы данных типа «ключ — значение», почти полный аналог Azure Table Storage;\n• Cassandra — сервис, являющийся адаптацией Apache Cassandra.\nУказанные выше базы собраны в единый сервис, предоставляющий всем им ряд общих уникальных черт и свойств. Прежде всего, это глобальная доступность — все перечисленные БД можно реплицировать во все регионы, то есть создать копии баз во всех регионах одновременно! Ну или только в выбранных. При этом в зависимости от географического местоположения клиента его запрос будет направлен к ближайшему дата- центру. Уровень согласованности данных можно настроить вплоть до уровня сильной согласованности (strong consistency). Кроме того, полоса пропускания и размер базы данных могут быть настроены независимо или сконфигурированы на автоматическое расширение. Уровень доступности для этого сервиса (для чтения) гарантируется на уровне 99,99 % применительно к одиночной БД без географической репликации и 99,999 % для репликации в нескольких географических регионах. При этом медианное время задержки в документации указывается равным 5 мс.\nПознакомимся подробнее с сервисом Azure CosmosDB и его возможностями, а затем перейдем к частным типам БД. Прежде всего необходимо создать аккаунт CosmosDB, в рамках которого мы будем создавать базы, а позже и сущности хранения в них. Для создания аккаунта нужно в левом верхнем углу портала нажать на ссылку + New и в появившемся окне поиска выбрать CosmosDB\nПосле нажатия кнопки Create (Создать) появится следующая форма\n \n Прежде всего здесь необходимо указать идентификатор аккаунта (ID), выбрать его тип (API), группу ресурсов (Resource Group), местоположение и включить опцию географического дублирования (Enable georedundancy). Список доступных API в развернутом виде представлен на правой половине рисунка. Вас не должна смущать аббревиатура SQL: имеется в виду база данных SQL API, которая, в свою очередь, соответствует DocumentDB (вероятно, такое странное название было дано в связи с тем, что DocumentDB допускает выполнение запросов к данным, написанным с использованием SQLподобного синтаксиса). Таким образом, при создании нужно выбрать SQL API и нажать кнопку Create (Создать).\nПосле создания аккаунта CosmosDB типа SQL API будет доступна стартовая страница. на Для всех типов поддерживаемых баз данных она выглядит примерно одинаково. Добавить коллекцию можно прямо на стартовой странице, но мы пойдем на вкладку Overview.\n \nПерейдя на вкладку Overview, можно видеть на карте доступные для репликации регионы (прозрачные шестиугольники) и регионы, использованные в настоящий момент (закрашенные шестиугольники). Кроме карты, на этой странице доступна панель мониторинга, которая размещается внизу экрана и не поместилась на снимке.\nЧтобы добавить коллекцию, необходимо нажать ссылку + Add Collection в верхней части экрана, после чего откроется форма, показанная на рис. 6.9. Поскольку\nу нас еще нет баз данных, нужно создать первую базу данных (Database id) —и, далее, в ее рамках создать коллекцию (Collection id) — GameEvents.\nДалее следует выбрать размер хранилища (Storage capacity). Он может быть фиксированным (опция Fixed (10 GB)) и неограниченным (опция Unlimited), то есть автоматически масштабируемым. Кроме того, нужно выбрать производительность (выражена в терминах единиц чтения в секунду) из заданного диапазона. Минимальное значение равно 400. Далее можно добавить в коллекцию документов уникальный ключ (ссылка + Add unique key) и нажать OK.\nТеперь вернемся структуре DocumentDB (а это не что иное, как SQL API). Аккаунты CosmosDB точно так же состоят из одной или нескольких баз данных, которые, в свою очередь, состоят из одной или нескольких коллекций. Эти два элемента сервиса CosmosDB SQL API мы и создали на предыдущем шаге. Каждая коллекция, в свою очередь, состоит из хранимых процедур (stored procedure), определяемых пользователем функций (user defined functions), триггеров (triggers) и собственно документов (documents). Разберем подробнее, что такое документ в терминах DocumentDB, на примере его создания. Для этого на вкладке Data Explorer перейдем к нашим созданным базе данных и коллекции и нажмем ссылку Documents. На появившейся вкладке нажмем на ссылку New Document, в результате изображение на экране будет выглядеть следующим образом.\nИтак, после нажатия на ссылке New Document создается JSONфайл с единственным обязательным полем, равным id. Значение этого поля содержит призыв на английском, который переводится как «замени новым document_id». Это первичный ключ, который должен быть уникальным. В данный документ можно добавить произвольное количество полей, определяемых пользователем. Необходимые изменения, внесенные в документ, показаны в средней части рисунка: первичный ключ равен 1, а пользовательское поле \"custom_key\" — 2. Далее следует нажать ссылку Save. Список созданных документов, для которых первоначально отображаются только ключи, приведен прямо на этой вкладке (под символом id). Если нажать один из этих ключей, то в окне отобразится документ, содержащий наши поля (ключ и пользовательское поле) и еще ряд служебных полей, что видно в нижней части рисунка. Итак, документы — это элементарные порции информации, хранящиеся в DocumentDB в формате\n\nJSON. Как было указано выше, большие данные чаще всего представляют собой большое количество элементарных порций данных, хранящихся в том или ином виде, так вот в DocumentDB элементарной порцией является документ JSON.\nТеперь приведем один пример применения SQLподобного запроса для выборки данных. Сначала нужно добавить несколько документов с различными значениями пользовательского поля.\nСтоит отметить, что, помимо JSONдокументов, документоориентированная база данных позволяет сохранять приложения (attachments) в виде двоичных файлов размером до 2 Гбайт, которые могут быть медиафайлами, архивами и др. (см. свойство \"_attachments\" в нижней части).\nПользовательские функции и хранимые процедуры — написанные на языке JavaScript элементы расширения стандартного синтаксиса запросов, которые\n  \n будут подробнее описаны в части III. Аналоги этих элементов — хранимые процедуры и функции в реляционных базах данных.\nТриггеры представляют собой программные конструкции, написанные на языке JavaScript, вызываемые при выполнении операций на документах. Триггер может выполняться непосредственно перед созданием документа (pretrigger) и после (posttrigger). Очень интересным и важным примером использования триггеров является интеграция с Azure Function — сервисами бессерверного исполнения кода, который может выполняться в ответ на внешний сигнал. Для CosmosDB возможны следующие сценарии запуска (в настоящее время реализованы только для SQL API и Graph API).\nПрименение потока событий изменения (change feed) — перехват Azure Function событий изменений коллекции (добавление/удаление/изменения). При этом поток событий активизирует пользовательский триггер, который, в свою очередь, запускает Azure Function. Данный подход полностью соответствует концепции EventDriven\nИспользование потока событий изменений для вызова Azure Function\nПрименение привязки входа (input binding) — при этом Azure Function читает данные из базы при запуске сторонним триггером (рис. 6.14).\nИспользование Azure Function с привязкой входа\nПрименение привязки выхода (output binding) — в этом случае Azure Function записывает данные в базу в ответ на запуск триггером (рис. 6.15).\nИспользование Azure Function с привязкой выхода\n   \nЭти паттерны полезны в реальных сценариях и позволяют строить сложные приложения, ориентированные как на события (Event Driven), так и на данные (Data Driven).\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 12.\n7.1. Общий обзор реляционных хранилищ данных\nРеляционная база данных предназначена для выполнения двух основных функций: OLAP и OLTP. Преимущественно РБД используются для целей OLTP. Это приводит к тому, что, с одной стороны, обычные реляционные базы имеют ограничения по размеру, а с другой — выполнить высокопроизводительные запросы к большим массивам данных может быть затруднительно.\nРассмотрим пример. Предположим, есть база данных SQL, которая служит как хранилище данных веб-приложения, непрерывно обновляемых пользователями. Кроме того, необходимо периодически строить отчеты в системах бизнесаналитики. Для этого нужно выполнить запросы, очень сильно нагружающие SQLсервер, поскольку они будут содержать операции группирования, совершения математических операций, просмотра множества таблиц и соединения в одном запросе результатов просмотра и пр. И тут возможна ситуация, когда запросы от подсистемы бизнес-аналитики могут очень мешать работе веб-приложения и даже блокировать ее из-за зависания базы данных во время выполнения BI запросов. Это происходит потому, что для выполнения таких запросов требуются гораздо большие вычислительные возможности, чем для обычной работы приложения.\nПроблема нехватки ресурсов баз может обостриться ввиду необходимости выполнять запросы к таблицам из разных БД. Она частично устраняется с помощью поднятия ценового уровня базы (для Azure SQL) или типа экземпляра (для AWS RDS) перед запуском всех BI запросов и опускания после их завершения. Но такой подход ограничен тем, что нужно разделить по времени периоды работы сервисов бизнес-аналитики и основного приложения. Кроме того, нерешенной является проблема выполнения запросов среди многих таблиц многих внешних БД. Да и предельно высокий ценовой уровень одиночной базы может оказаться недостаточным. Как быть в этом случае? Вот тут-то на помощь и приходит DWH (Data Warehouse) — «склад данных» — реляционное хранилище данных, оптимизированное для хранения огромных массивов данных и выполнения запросов к ним\n\n Наполнение хранилища Data Warehouse и доступ сторонних сервисов к нему\nВысокая производительность аналитических запросов достигается благодаря массивнопараллельной архитектуре и распараллеливанию запроса для выполнения несколькими серверами одновременно. В данном случае тоже необходимо разместить информацию из различных БД в одном общем сервисе — «складе» (DWH). Преимущество такого подхода в том, что SQL DWH позволяет использовать стандартные SQLзапросы, хранимые процедуры и др., а следовательно, такое хранилище совместимо со всеми стандартными средствами и инструментами BI.\nРеляционное хранилище данных (SQL DWH) представляет собой массивнопараллельное хранилище, состоящее из кластера серверов трех типов: головного (управляющего), узлов вычисления и томов хранения\n Общая архитектура реляционного хранилища данных\n\nКлиент (а точнее, программный клиент) может взаимодействовать с реляционным хранилищем только через головной узел. Последний отвечает за распределение данных по вычислительным узлам, в ряде случаев являющимся одновременно узлами хранения. Кроме того, головной узел компилирует запросы языка SQL в команды кластеру, которые обеспечивают параллельное выполнение, а затем собирает результат вместе и выдает назад клиенту. Реляционное хранилище данных, в отличие от реляционных баз данных, оптимизировано только для целей OLAP. Выполнение транзакций в DWH возможно, но это несвойственно хранилищу, основная задача которого — хранить информацию и предоставлять доступ к ней с помощью интерфейса SQL. При этом производительность запросов на чтение данных существенно выше, чем для обычной БД за счет параллельного выполнения и иного физического представления данных, другого плана выполнения запросов и индексов.\nРассмотрим вопрос параллельного выполнения запросов. Необходимо разделить данные между вычислительными узлами. Это можно сделать следующими способами. Во-первых, в каждом узле разместить идентичные копии данных и разделить запрос таким образом, чтобы каждая его часть в вычислительном узле обрабатывала свой набор строк. Как уже отмечалось, за разделение запроса на подзапросы и соединение их результатов отвечает головной узел. Такая архитектура очень надежна: утеря одного узла никак не повлияет на информацию в хранилище. Недостаток данной архитектуры в том, что из-за ограниченного объема дисков одного узла в нем невозможно хранить очень большие объемы данных. Во-вторых, можно размещать данные в различных узлах без дублирования. При этом вероятно как псевдослучайное распределение строк данных по узлам, так и последовательное, в порядке роста значения ключа. В этом случае возможно хранение и обработка гораздо больших объемов данных, чем в случае их полного копирования в каждом узле.\nИтак, реляционное хранилище данных применяется, когда необходимо получить централизованное хранилище информации, позволяющее выполнять запросы с помощью стандартного синтаксиса SQL, поддерживаемого большинством систем BI. Для этого нужно извлечь данные из внешних источников (extract), преобразовать их к виду, удобному для добавления в хранилище с необходимой обработкой, фильтрацией и пр. (transform) и загрузить их в хранилище (load). Такой подход называется ETL и является традиционным. Он очень удобен в случае, когда внешние источники данных очень разнородны и все вместе или по отдельности не поддерживают встроенный механизм выполнения аналитических запросов. Недостаток ETL состоит в том, что для наполнения реляционного хранилища обязательно нужен сервис копирования и трансформации. Критически важный момент для ETL — метаданные о сторонних источниках: что в них содержится, в каком\n\nформате и др. Эти метаданные в облачных средах могут быть представлены в специальных сервисах — каталогах данных (data catalog).\nРассмотрим логическую структуру информации в реляционном хранилище. Ниже представлены несколько принципов организации реляционных хранилищ данных.\n• Данные в хранилище должны быть объединены в соответствии с категориями предметной области, а не их физическими источниками.\n• Централизованное хранилище должно хранить в своем составе данные, относящиеся ко всей системе в целом, а не только к отдельным аспектам. Только в таком случае можно построить комплексные аналитические запросы. Данные в хранилище содержатся и поступают извне, но не создаются.\n• Говорить о корректности данных в хранилище можно только с привязкой их к конкретному временному промежутку.\nСуществует две стратегии наполнения хранилища данными: стратегия полного обновления (каждый раз происходит полное обновление данных из внешних источников) и инкрементального (обновления только тех данных, которые были изменены в процессе работы OLTP подсистемы). В плане реализации, безусловно, проще стратегия полного обновления, в то время как инкрементальное требует использования сложных ETL процедур замены обновленных элементов, вставки новых и удаления тех, что были удалены в OLTP. Выбрать ту или иную стратегию можно только путем совместного целостного анализа хранилища данных, их источников и характера их изменений.\nЕсть две общие модели построения реляционных хранилищ: нормализованные хранилища и хранилища с измерениями. Построение нормализованного хранилища в общем не отличается от построения нормализованной реляционной базы данных. Информация хранится в связанных таблицах в третьей нормальной форме, и, как следствие, требуется выполнение выборки из многих таблиц, что потенциально ведет к усложнению запросов и падению производительности. Хранилище с измерениями — это денормализованная форма хранения информации. В свою очередь, возможны две архитектуры хранилища с измерениями — в виде схем «звезда» и «снежинка».\nВсхеме «звезда» информация хранится в таблицах двух типов: центральной таблицы фактов и многочисленных таблиц измерений .\n\n Архитектура хранилища данных в виде схемы «звезда»\nв центре звездообразной структуры находится таблица фактов, содержащая информацию, которая будет анализироваться. Она может включать факты (а по сути, записи), связанные с событиями или состоянием объекта, с транзакциями и пр. Таблица содержит первичный ключ, числовые поля, характеризующие данный факт, и внешние ключи, ссылающиеся на таблицу измерений. Числовые поля должны включать аддитивные значения, подлежащие анализу (в том числе суммированию, вычислению среднего значения и др.). Таблицы измерений нужны для хранения атрибутов фактов (как правило, текстовых), которые не могут быть использованы для численных операций со строками таблицы фактов.\nв Например, в таблице фактов с данными о проведенных покерных играх каждая строка содержит сумму выигрыша игрока, внешний ключ на таблицы измерений игроков и игр. В схеме «звезда» данные существенно денормализованы для оптимизации запросов.\nИногда требуется добавить элемент нормализации и расщепить некоторые таблицы изменений на несколько таблиц — это схема «снежинка» .\n Архитектура хранилища данных в виде схемы «снежинка»\n\nВыбор той или иной структуры реляционного хранилища данных и построение запросов (OLAP — кубы, DataMart — витрины данных) является чисто прикладной задачей и архитектура и принцип организации хранилища напрямую зависит от целей его создания и предполагаемого способа использования.\nAzure SQL DWH\nРеляционное хранилище Azure SQL DWH — облачный сервис от Microsoft Azure, построенный на основе сервиса Azure SQL. Этот сервис использует технологию Microsoft PolyBase для построения запросов к данным, расположенным как в хранилище Azure Blob Storage, так и в базах данных Azure SQL. Рассмотрим основные концепции сервиса Azure SQL DWH.\nАрхитектура хранилища Azure SQL DWH соответствует общей архитектуре реляционного храниища, рассмотренной ранее. Как головной узел, так и вычислительные узлы содержат экземпляры системного сервиса DMS (Data Movement Service), отвечающего за передачу данных между узлами. Данные физически хранятся в Azure Storage. Такое физическое разделение их хранения и параллельного выполнения запросов обеспечивает возможность независимого масштабирования объемов хранения и вычислительных мощностей кластера.\nНепосредственно из Azure Storage данные разделяются по вычислительным узлам с помощью одного из трех видов распределения (distributions). Каждый запрос разделяется на 60 меньших фрагментов, каждая из которых выполняется на вычислительном узле. Каждый вычислительный узел работает с 1 до 60 фрагментов в зависимости от его ценового уровня. На наивысшем уровне (максимальная производительность вычислительного узла) используется одно распределение на узел, при минимальных ресурсах — все распределения на один узел.\nСуществует три возможных паттерна данных, влияющих на производительность запросов:\n• распределение с помощью хешфункции (hash);\n• последовательное, или циклическое, распределение (round\nrobin);\n• репликация (replicate).\nИдея использования хешфункции для распределения строк из исходной таблицы по вычислительным узлам состоит в том, что строки равномерно, независимо и случайно разделяются по вычислительным узлам (рис. 7.5). Это позволяет строить оптимальные запросы с помощью соединения таблиц и\n\nагрегации данных. При этом каждая строка таблицы относится к одному распределению.\n Распределение данных таблицы по вычислительным узлам с помощью хешфункции\nЦиклическое распределение строк по вычислительным узлам подразумевает последовательное равномерное наполнение узлов со случайным выбором начального узла. Такое распределение обеспечивает высокую производительность операций выборки данных в случае, когда в соединении таблиц нет нужды, поскольку оно потребует выполнения дополнительных «перетасовок» строк между узлами.\nРепликация применяется при наличии таблиц небольшого размера и состоит том, что такая таблица копируется на все вычислительные узлы. Это позволяет добиться высокой производительности любого типа запроса и одновременно увеличить надежность системы, поскольку отказ одного вычислительного узла или более повлияет лишь на производительность.\n Репликация таблицы по вычислительным узлам\n\nТеперь посмотрим, как работать с Azure SQL DWH из веб-портала. Для этого во вкладке добавления ресурсов в строке поиска следует написать SQL Data Warehouse.\nПосле нажатия кнопки Create (Создать) откроется следующая форма настройки реляционного хранилища Azure SQL DWH.\nЗдесь нужно указать имя базы данных (Database name), выбрать ресурсную группу ( Resource group), сервер (Server) и ценовой уровень производительности (Performance tier).\nДля реляционного хранилища можно настроить ценовой уровень по двум разным критериям: гибкости (Optimized for Elasticity) и вычислительным возможностям (Optimized for Compute). Дадим пояснение величинам, используемым для оценки производительности реляционного хранилища — DWU и cDWU:\n• DWU (Data Warehouse Unit) представляет собой абстрактную величину, описывающую в нормализованном виде доступные вычислительные ресурсы: CPU, память и полоса пропускания дисков данных — IOPS;\n• cDWU (compute Data Warehouse Unit) — с одной стороны, описывает скорость чтения информации из Azure Storage, а с другой — скорость выполнения запросов.\n \nПосле создания Azure SQL DWH панель будет отображать основные возможности Azure SQL DWH, которые близки к таковым у базы данных Azure SQL. На панели представлены отдельный сервис загрузки данных — Load Data (поддерживается загрузка с помощью RedGate и Azure DataFactory), интеграция с сервисом SSAS (ссылка Model and Cache Data), сервис бизнесаналитики (ссылка Open in PowerBI) и, конечно, сервисы мониторинга (Monitoring), масштабирования (Scale), выполнения запросов в онлайн- редакторе (ссылка Open Editor). Кроме того, доступны опции бэкапа, аудита и шифрования.\nВ качестве движка сервиса Azure SQL DWH используется Microsoft SQL DWH и, соответственно, язык запросов TSQL. Чтобы подключить внешний клиент, следует задействовать строку подключения, доступную по ссылке Connection String. Наиболее полная интеграция возможна со средствами бизнес-анализа и аналитики, предоставляемыми корпорацией Microsoft.\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 13.\nАнализ данных состоит в их выборке, агрегировании или ином преобразовании, делающих информацию удобной для изучения. Действительно, сырые данные, будь то большие данные или просто данные из файлов логов, человеку трудно прочесть. Они представляют собой, по сути, таблицы или структурированные файлы формата JSON/XML, и, просто просматривая их, можно извлечь чрезвычайно мало информации в силу психофизиологических особенностей человека.\nАнализ данных может быть следующих видов:\n• потоковый — применяется для анализа потоков данных в реальном режиме или с небольшой задержкой;\n• интерактивный — служит для анализа данных, находящихся в хранилище, при активном участии пользователя; последний при этом вводит запрос на вебпортале или в терминале и ожидает ответ в течение минимального времени;\n• пакетный — используется для выполнения периодических задач анализа в хранилище данных, например агрегирования, фильтрации и др.;\n• интеллектуальный — это анализ данных с применением сложных алгоритмов машинного обучения. Я задействовал термин «сложных», поскольку во всех предыдущих случаях алгоритм анализа пользователь пишет сам. В случае же интеллектуального анализа этот алгоритм, а вернее, его параметры определяются путем процедуры обучения непосредственно на самих данных. Интеллектуальным этот анализ называется ввиду того, что в нем применяются элементы искусственного интеллекта, например машинное обучение с помощью алгоритмов на основе нейронных сетей. Сервисы интеллектуального анализа данных — это темы отдельной книги и здесь подробно рассматриваться не будут.\nКак правило, указанные виды сервисов применяются в той или иной комбинации. Очень интересный пример их комбинации — система анализа финансовых онлайнтранзакций банковских карточек на предмет обнаружения подозрительных действий (выдача большой суммы наличных в другой стране или выдача малых сумм наличных в различных банкоматах в малый промежуток времени и пр.). Для выявления таких действий необходимо анализировать поток сообщений, выделить из него сообщения, относящиеся к одной карточке и потоковому анализу.\n\nИ здесь возможны несколько вариантов. Например, предварительное накопление сведений о транзакциях в хранилище и применение сервисов интерактивного анализа для предварительного изучения данной информации. Затем на основании этого изучения строится система пакетного анализа, которая выделяет существенные признаки, характеризующие типовое поведение держателя карточки. Они служат для дальнейшей корректировки алгоритмов (а точнее, численных индексов в алгоритмах) и потокового анализа.\nГлавная идея такой архитектуры — изучение закономерностей поведения конкретных держателей карточек с целью определить их финансовые паттерны поведения. Эти паттерны выражаются в конечном итоге в числовых или исчислимых показателях (величина дневной суммы, снимаемой с карточки; страны, в которых снимается сумма), используемых для корректировки соответствующих параметров у систем потокового анализа. Но тут очевиден существенный недостаток: если паттерны будут найдены и жестко закодированы в виде алгоритма на компилируемом языке, то адаптировать его для конкретного специфичного пользователя не представляется возможным.\nНивелировать этот недостаток поможет применение машинного обучения, которое в общем делает то же самое — «обучается» на выборке данных, строит модель, отражающую в себе поведение пользователя (например, это могут быть «обученная» нейронная сеть или весовые коэффициенты связей персептрона и пр.). Потоковый анализ в данном случае будет состоять в подаче на вход такой модели входного потока. Ее выход имеет два состояния: «транзакция правильная» и «транзакция подозрительная». Модель можно периодически обновлять — уточнять ее на новом наборе данных. Но это может повлечь значительные трудности, связанные с возможностью переобучения, то есть построения неверной модели на переизбытке данных. Вообще, тема интеллектуального анализа данных и облачных сервисов, предоставляющих инструменты для его проведения, очень обширна и требует отдельной книги, а потому далее рассматриваться не будет. Я надеюсь, вы не обидитесь на меня за это. Во введении было указано, что машинного обучения в книге практически не будет. Если читатели проявят интерес к данной теме — такая книга, безусловно, появится.\nСами алгоритмы анализа в различных системах можно описать тремя разными способами:\n• в виде сценария SQL-подобного синтаксиса;\n• с помощью специализированного синтаксиса запросов, отличного от SQL;\n\n• через программу на компилируемом или интерпретируемом языке программирования общего назначения, которая использует тот или иной фреймворк обработки данных.\nРассмотрим эти способы более подробно.\nИспользование SQL для обработки данных различной природы получило распространение совсем недавно. Традиционно для этой цели служило «складирование» данных в одно общее реляционное хранилище (DWH), ввиду чего они извлекались из своих источников, подвергались трансформации и помещались в таблицы SQL DWH. Затем к этим данным можно применить запросы SQL и сделать доступными для традиционных средств BI. В последнее время появились сервисы, позволяющие применять SQL-подобный синтаксис к данным, хранящимся в HDFS (SparkSQL, HiveQL, AWS Athena SQL, AWS Redshift Spectrum, Azure Data Lake Analytics), и к потоковым данным (AWS Kinesis Analytics, Azure Stream Analytics). Такой подход очень удобен, поскольку запросы, используемые при интерактивном анализе данных, можно задействовать напрямую для построения задач пакетного и потокового анализа. Кроме того, BigData разработчик, работающий со всеми этими аспектами обработки данных, должен хорошо знать SQL, а не три совершенно не связанных между собой синтаксиса или фреймворка языка программирования общего назначения.\nСледующий способ описания алгоритмов обработки данных — специализированные языки манипулирования данными. Некоторые из них имеют достаточно давнюю историю и применялись для обработки данных до того, как появился сам термин «большие данные», хотя, безусловно, физически большие объемы данных, требующие обработки, существуют достаточно давно. В качестве примера можно назвать такие языки (а в ряде случаев скорее целые экосистемы), как PigLatin, R, MatLab, SPSS, Elasticsearch Query, PowerBI query language и др. Некоторые стали дефакто стандартом обработки данных (например, R) и представлены в облачных средах в виде готовых сервисов (Azure HDInsight RServer, Power BI) или образов виртуальной машины. Подобные языки используются для интерактивного анализа данных в системах аналитики или BI. Сам анализ производится в специализированных серверах аналитики, к которым подключены источники данных. В отличие от этих языков PigLatin в облачных системах служит в качестве альтернативы программной реализации паттерна MapReduce для целей пакетной обработки данных (чаще всего это процедуры ETL/ELT) в среде Apache Pig.\nОсобняком среди этих языков стоит язык запросов Elasticsearch. В настоящее время он очень широко используются в стеке программных продуктов ELK (рис. IV.1), состоящем из следующих компонентов: Elasticsearch — кластер системы хранения и индексирования информации;\n\nLogstash — набор коннекторов, обеспечивающих поступление информации в Elasticsearch; Kibana — система отображения информации в виде интерактивных графиков.\nИнформация добавляется в Elasticsearch путем RESTзапросов, которые могут быть посланы с помощью сервисов Logstash, пользовательскими программами или сценариями командной оболочки. Непосредственно Logstash представляет собой\nОбщая архитектура стека ELK\nспециализированный сервис стека ELK, устанавливаемый на серверыисточники логов или специализированные серверы с доступом к логам источников. Logstash отвечает за захват логов из источников, их фильтрацию и преобразование и перенаправление в приемник — кластер Elasticsearch. Для обеспечения парсинга логов источника служит специальный синтаксис и расширение — Grok.\nПосле добавления информации в кластер Elasticsearch и ee индексирования (то есть упорядочения с помощью структуры данных, обеспечивающей высокую скорость поиска) она становится доступна для поиска через запросы. В основе Elasticsearch лежит Apache Lucene — поисковый движок, отвечающий за построе ние массивнопараллельной системы хранения и поиска информации. Помимо поиска, информация в Elasticsearch доступна для анализа путем языка запросов и отображена с помощью Kibana.\nСтек ELK в облачных средах в настоящее время используется в качестве системы централизованного хранилища и анализа логов, к которой с помощью\n \nLogstash можно подключить различные хранилища. ELK настолько популярен, что, например, в облачном провайдере AWS представлен как сервис AWS Elasticsearch Service. Кроме того, возможна установка отдельных компонентов в виде контейнеров Docker в сервисах кластеров, предоставляемых облачными провайдерами.\nИнаконец, для данных можно применить языки программирования общего назначения, которые вместе со специальными библиотеками компилируются (или транслируются, если это интерпретируемый язык) в исполняемые программы, запускаемые в соответствующем сервисе. Чаще всего в качестве таких языков выступают Java, Scala и Python. По сравнению с SQL и доменноспецифичными языками они имеют преимущества, связанные с очень большой гибкостью и разнообразием прикладных библиотек. Java, Scala и Python позволяют (по крайней мере потенциально) реализовать любой сколь угодно сложный алгоритм анализа данных. В то же время реализация типовых конструкций анализа (фильтрация, агрегирование, соединение и пр.) требует достаточно большого количества строк кода. Кроме того, люди, не имеющие опыта разработки и отладки программ на языках программирования общего назначения, могут испытывать сложности, применяя сервисы анализа, основанные на этих языках.\nРяд сервисов анализа данных (например, система Spark) предоставляет внешний веб-интерфейс, в котором можно применять Python в интерактивном режиме. Мы уже встречались с этим фактом, рассматривая выполнение ETL с помощью AWS Glue. Другие сервисы, такие как Hadoop MapReduce, требуют создания программ на компилируемом языке Java..\nКак уже отмечалось ранее, цель всего анализа данных — не получение новой информации, а поиск и отображение существующей, скрытой в большом объеме данных. Фильтрация, группировка, отображение, с одной стороны, и соединение с объединением — с другой, — это лишь способы предоставления существующей информации в виде, самом удобном и наглядном для интерпретации. Зачастую такой подход называется data mining («добывание информации») — по аналогии добычей полезного минерала из пустой породы, тоже состоящей из минералов, но не представляющей пока интереса для анализа.\nОсобенно хорошо этот подход изучать на примере аналогии — добычи золота. Прежде чем начать добычу на том или ином месторождении, геологоразведчик анализирует образцы из различных проб, чтобы оценить концентрацию золота (аналог интерактивного анализа). Как только определено, что золото в пробах есть, следует оценить экономическую целесообразность его добычи. Для этого необходимо, чтобы концентрация золота на месторождении была достаточно высокой. (Пороговое значение зависит от транспортной и общей освоенности территории вокруг\n\nпредполагаемого месторождения: чем более глухое и труднодоступное место, тем выше должна быть концентрация золота, чтобы его было выгодно осваивать.) После того как установлена целесообразность добычи, начинается собственно процесс разработки месторождения (аналог пакетного анализа). При этом чаще всего происходит многостадийный процесс обогащения и очищения, для чего порой золотоносная порода после первоначальной обработки транспортируется за пределы своего первоначального источника (аналог ETL) или (как, например, в случае технологии кучного выщелачивания) транспортируется без предварительного обогащения к месту обработки и уже там подвергается действию цианидных реагентов (аналог ELT). Кроме того, золото можно добыть (хотя бы потенциально) в качестве попутного компонента в других технологических процессах: рафинирования меди, из вулканических вод или непосредственно из морской воды (аналог потокового анализа). Но в ряде случаев происходит стихийная добыча старателями, которые мечтают найти свой огромный самородок. Часто кластерные системы анализа данных используются именно в таком режиме: проанализировать логи за месяц в поисках 500 ошибок, определить причину странного поведения системы (падения производительности, увеличения времени отклика и т. д.) и пр.\nОбщие сведения об интерактивном анализе данных\nИнтерактивный анализ предполагает активное участие специалиста по обработке данных (data scientist). Этот человек выдвигает гипотезы, выбирает программные инструменты анализа, определяет источники данных, применяет к ним запросы на языке, который поддерживают эти инструменты. Сервисы интерактивного анализа предоставляют средства отображения информации в виде таблиц или графиков. Такой анализ применяется, когда надо изучить данные в ответ на однократный запрос бизнеса. Например, установить тенденцию в бизнес-данных, не отображаемую периодических отчетах. Трудность интерактивного анализа заключается в том, что требуется создать систему, которая позволяет выполнять запросы пользователя к большому объему данных (речь идет не о петабайтах, а «всего лишь» о многих гигабайтах) за достаточно малое время, исчисляемое минутами. Это необходимо для того, чтобы работа специалиста по обработке данных была максимально эффективной и цепочка «выдвижение гипотез — написание запроса — выполнение запроса — анализ результата — корректировка гипотез...» занимала наименьшее время. Очевидно, что для построения такой системы подойдут ресурсы или сервисы, создаваемые по требованию. Как правило, они состоят из хранилища данных, допускающего быстрый анализ; кластера, предоставляющего вычислительные ресурсы для построения запроса и выполнения параллельных вычислений; сервисов копирования и трансформации данных, служащих для перемещения информации из сторонних источников в это хранилище.\n\nАнализ реляционных данных\nНаиболее распространенная модель хранения данных в различных информационных системах — реляционная, в которой информация логически представлена в виде связанных таблиц. Для выборки и обновления информации в этих таблицах используется специальный стандартный язык запросов — SQL (Structured Query Language). Его поддерживают очень многие базы данных, хранилища и средства аналитики, потому рассмотрим его подробнее. Следует заметить, что, несмотря на единообразие и широкое распространение SQL как стандарта, существует множество различных синтаксисов этого языка, являющихся его конкретными реализа циями (например, TSQL от Microsoft, PL/SQL от Oracle, PostgreSQL и пр.). Разница между ними проявляется в различных поддерживаемых типах данных, применении встроенных функций, расширяемости пользовательских типов данных, процедурах и функциях, а также в различной поддержке стандартных синтаксических конструкций. Тем не менее, зная базисные элементы SQL и принципы построения запросов, вы относительно легко можете освоить любой синтаксис SQL от любого производителя.\nДля взаимодействия с реляционной БД или реляционным хранилищем требуется внешняя программа с графическим интерфейсом редактора, например SQL Management Studio для Microsoft SQL Server или SQL Workbench для PostgreSQL. Облачные среды порой предоставляют онлайн- интерфейсы редакторов запросов.\nAzure Data Lake Analytics\nСервис нереляционного хранилища данных Azure Data Lake предоставляет встроенный сервис построения SQLзапросов к данным, расположенным в нем. Синтаксис запросов, реализованный в этом сервисе, называется USQL. Сами запросы выполняются как задания (jobs), и начисление платы в этом случае идет за транзакции чтения и записи, то есть за фактическое выполнение, а не хостинг. Подробнее рассмотрим этот язык.\nUSQL представляет собой комбинацию языков SQL и C#, оптимизированную для выполнения запросов к данным, хранящимся в Azure Data Lake Store. Такая комбинация позволяет работать со слабоструктурированными данными с отсутствием общей схемы или с гетерогенной схемой. Запросы USQL можно выполнять двумя способами: локальное исполнение на вашем компьютере (USQL Local Execution) и выполнение в сервисе Azure Data Lake или исполнение в облаке (USQL Cloud Execution). В практическом примере мы рассмотрим оба варианта, пока же акцентирую внимание на синтаксисе и прежде всего перечислю его ключевые компоненты.\n\n• Переменная набора строк (rowset variable) — переменная, хранящая результат выполнения запроса, содержащая строки. Каждая такая переменная начинается с символа @, за которым следует ее имя: @variableName. В качестве источников данных для переменной набора строк выступает оператор извлечения или другая переменная набора строк.\n• Оператор извлечения (EXTRACT) предназначен для чтения данных из файлов и парсинга их схемы данных. По умолчанию позволяет читать текстовые данные с разделением табуляцией. Можно кастомизировать этот оператор, создав свой экстрактор.\n• Оператор вывода (OUTPUT) отвечает за запись данных из переменной набора строк в файл. Создает файлы, в которых колонки в строках разделены запятыми. Этот оператор тоже можно кастомизировать.\n• Пути файлов — оба оператора, извлечения и вывода, оперируют с файлами, расположенными в Azure Data Lake Store. Эти файлы расположены в иерархической структуре каталогов. Различие между путями файлов и локальными путями в том, что в качестве корневого каталога выступает аккаунт Azure Data Lake, относительно которого задается файловая структура.\n• Скалярные переменные — переменные, содержащие скалярную величину определенного типа (INT, VARCHAR, DATETIME). Подчиняются тем же синтаксическим правилам, что и скалярные переменные TSQL.\n• Операторы преобразования переменных набора строк, агрегации и фильтрации — основные операторы выполнения запросов.\nТеперь рассмотрим конкретный пример. Прежде всего изучим, как создавать файл, заполнив его с помощью запроса, выполняемого в облаке. Создадим экземпляр Azure Data Lake Analytics, для чего в строке поиска Marketplace введем Data Lake Analytics. В результате появится форма, в которой следует нажать кнопку Create (Создать).\nПосле этого откроется форма конфигурирования сервиса:\n\n Форма конфигурирования аккаунта Data Lake Analytics\nВ этой форме нужно указать имя аккаунта, которое будет префиксом адреса аккаунта, выбрать существующую ресурсную группу, указать регион размещения сервиса (East US 2). После этого стоит добавить аккаунт Azure Data Lake Store. После завершения конфигурирования следует нажать кнопку Create (Создать).\nПосле успешного выполнения описанных действий можно открыть общую панель сервиса. Характерной особенностью этого сервиса является то, что доступ к нему контролируется с помощью Azure Active Directory и можно добавлять пользователей прямо через мастер добавления, вызываемый нажатием на ссылку Add user wizard, после чего настраиваются их права.\n",
    "Принципы построения, проектирования и эксплуатации ИС. Лекция 14.\nТеперь посмотрим, как использовать Azure Data Lake Analytics. Этот сервис работает с файлами в Azure Data Lake Store и Azure Blob Storage, которые должны быть туда помещены. Затем результат работы исполняемых заданий Azure Data Lake Analytics также сохраняется в новый файл в этом хранилище. Данный подход именуется ELT (Extract Load Transform — «извлечение, загрузка, преобразование»). Он принципиально отличается от традиционного подхода ETL, характерного для реляционного хранилища данных. Итак, результатами выполнения запросов являются новые файлы. С помощью встроенных операторов можно работать с форматами CSV и TSV. Последний представляет собой текстовый формат, при котором колонки разделяются символами табуляции, а строки — символами перевода строки.\nСоздать файл проще всего с помощью выполняемого сценария. Чтобы создать выполняемое задание, следует нажать на ссылку + New job, после чего откроется форма онлайн-редактора запросов. Редактор со сценарием, создающим новый файл и заполняющий его информацией,. В данном редакторе указывается имя выполняемого задания (Job name), определяется положение ползунка выбора уровня производительности AUs (analytics units) — обобщенных характеристик вычислительных ресурсов, выделяемых на выполнение сценария. Важный параметр выполнения — приблизительная стоимость задания, поскольку очень большие объемы данных занимают длительное время выполнения сценария и выдвигают высокие требования к ресурсам, что соответственно влечет затраты денежных средств. Сам сценарий можно сохранить на диск (ссылка Save as). Можно добавить файл с диска (Open file). Чтобы запустить выполняемое задание, следует нажать кнопку Submit (Отправить).\n Внешний вид редактора со сценарием создания и заполнения CSV файла\n\nДанный сценарий включает ряд компонентов. Переменная @insertedRows после выполнения сценария станет содержать набор строк, которые будут вставляться в файл. Синтаксис заполнения переменной набора строк включает в себя знакомые нам операторы создания новых строк и вставки этих строк в переменную @insertedRows. Далее данные из этой переменной с помощью оператора OUTPUT помещаются в выходной канал. В этом случае выходным каналом является CSVфайл \"/TestData.csv\", который создается с помощью оператора Outputters.Csv().\nКогда задание начнет выполняться, в портале откроется страница, на которой показаны этапы исполнения сценария в виде графа, а также приведены основные метрики: использование AU, приблизительная потраченная сумма, время исполнения, эффективность, приоритет и др.. Результат исполнения в виде данных, добавленных в файл, можно посмотреть на вкладке Data\nПанель мониторинга выполняемого задания\nВкладка Data, где доступны входные и выходные источники\n  \n Глава 12. Интерактивный анализ данных\n365\n Обзор созданного файла\nОднако описанный выше способ создания файла — скорее методический, чем прикладной. В реальных проектах основным способом наполнения Azure Data Lake Store является использование Azure Data Factory. Для дальнейшего объяснения примера я загрузил файл Payments.tsv с помощью Azure Data Factory. Источником была база данных Azure SQL, а файл-приемник создавался с разделением колонок символами табуляции.\nПростейший сценарий, извлекающий данные из файла Payments.tsv и помещающий их без преобразования в файл Payments_copy.tsv:\n Сценарий прямого копирования информации из файла Payments.tsv\n\nВ этом сценарии объявлены две переменные, содержащие пути файлов: @InputFile (путь ко входному файлу) и @OutputFile (путь к выходному). Обратите внимание, что выходной файл может остутствовать на момент выполнения сценария, поскольку он вместе с вмещающей его папкой будет создан при успешном выполнении сценария. Тип данных — string, соответствующий типу System.string платформы .Net. Граф выполнения файла имеет следующий вид:\nДля выполнения полезной трансформации данных необходимо использовать операторы преобразования переменных наборов строк. К таковым может относиться фильтрация строк, агрегирование, выборка определенных колонок и др. Но в любом случае выходным источником будет новый файл.\nНачнем рассмотрение операторов трансформации с фильтрации и выбора заданного набора колонок. Пример подобного сценария:\n  Сценарий фильтрации данных\n\nПомимо известных нам операторов, мы видим операторы, выбирающие из входного набора строк только те, которые соответствуют AccountId, равному 1. Обратите внимание на синтаксис: для сравнения используется оператор ==.\nРезультат выполнения сценария трансформации переменной набора строк помещается в новую переменную @result, которая, в свою очередь, с помощью оператора OUTPUT сохраняется в выходном файле\nТут следует дать пояснение относительно имен полей данных. Дело в том, что данные сами по себе хранятся в файлах CSV или TSV без указания имен полей. Эти имена необходимы для использования переменных набора строк и создаются при работе извлекателя (Extractor). Далее в сценарии к ним можно получить доступ, обращаясь именно к переменной, а не к файлу. Но в выходном файле строки будут сохраняться опять без добавления имен полей.\nТеперь рассмотрим применение операторов группировки на примере сценария, суммирующего платежи для каждого аккаунта и выбирающего среди них те, для которых было выполнено не менее 1000 транзакций. Сценарий группировки:\nСценарий агрегирования значений, извлеченных из входного файла\nВозможности сервиса Azure Storage Analytics не ограничиваются сценарием «файлисточник — сценарий преобразования набора строк — файл-приемник», но предоставляет средство группировки всех объектов в нереляционное хранилище со структурой, концептуально повторяющей DWH. Это средство называется каталогом USQL (USQL Catalog). Каждый аккаунт Azure Data Lake\n \nAnalytics со держит ровно один каталог. Этот каталог состоит из одной или нескольких БД, вмещающих данные, организованные в таблицы, и код в виде пользовательских функций, объектов учетных данных и пр. Структура каталога:\nСтруктура каталога USQL\nБаза данных — логическая группа пользовательских объектов. Каталог содержит как минимум одну базу — master. Пользовательские БД создаются с помощью команды CREATE DATABASE IF NOT EXISTS DatabaseName, а удаляются командой DROP DATABASE IF EXISTS DataBaseName. Таблицы представляют собой способы организации данных. Их можно создать пустыми и заполнить позже или создать при выполнении запроса SELECT.\nПрежде чем создавать таблицу, посмотрим, как оборачивать сценарий извлечения строк из файла в пользовательскую табличную функцию (tablevalued function, TVF). Расположим ее в базе данных master. Сценарий создания такой функции приведен ниже:\n  \nТеперь создадим базу данных, таблицу Payments, расположенную в этой базе, причем посмотрим, как создавать ее во время выполнения оператора\nSELECT, вызываемого в пользовательской функции ExtractInformation. Второй вариант — создание таблицы DDL и заполнение ее с помощью созданной нами функции. Подобный сценарий имеет следующий вид:\nВ этом сценарии удаляется предыдущая версия базы данных, если такая существовала, далее создается новая с помощью операторов CREATE DATABASE. Затем оператор USR DATABASE переключает контекст на применение базы данных. Такое переключение позволяет использовать все объекты этой БД без указания полностью квалифицированного имени объекта, состоящего из имени базы и схемы. Далее следуют операторы DDL, удаляющие предыдущую таблицу Payments и создающую ее опять с применением оператора SELECT. Обратите внимание: таблица создается с кластерным индексом по имени sl_idx,\nараспределение строк по кластеру обеспечивается с помощью хешфункции. Нет необходимости явно указывать названия и типы полей, поскольку это происходит автоматически при выполнении оператора SELECT. Саму пользовательскую функцию ExtractInformation() нужно указать с полностью определенным именем: базой данных по умолчанию master и схемой по умолчанию dbo. После создания таблица доступна для выполнения прямых запросов точно таким же образом, как и в случае обычных таблиц в РБД. Но главное отличие от реляционных баз данных и DWH состоит в том, что результат выполнения запроса помещается в переменную набора строк, которая, в свою очередь, сохраняется в файл.\nСозданные таблицы позволяют строить практически такие же сложные запросы, как и в случае запросов в Azure SQL DWH, поскольку USQL поддерживает многие возможности подмножества DML TSQL, а именно, соединение (JOIN), объединение (UNION), оконные функции (WINDOW FUNCTIONS), View и пр. С другой стороны, таблицы дают возможность создавать пользовательские объекты и кастомизировать встроенные с\n \nпомощью .Net библиотеки, а также задействовать преобразования набора строк благодаря расширениям для языков Python и R.\nОсобняком стоят когнитивные расширения языка USQL (cognitive extensions), обеспечивающие обработку файлов изображений в целях распознавания приведенных на них объектов: текстовых и цифровых символов, лиц, эмоций, других объектов. Кроме того, возможен интеллектуальный анализ текста в файле в виде извлечения ключевых фраз и выполнения анализа общей тональности.\nСпомощью SDK для .Net и Visual Studio созданную базу данных можно экспортировать из другого аккаунта Azure Data Lake Analytics и импортировать в подобный аккаунт. Как видим, связка Azure Data Lake Store/Analytics обеспечивает построение нереляционного хранилища, позволяющего выполнять как интерактивный, так и пакетный анализ данных. Существует несколько возможных путей автоматизации запуска выполняемых заданий Azure Data Lake Analytics. Первый вариант: использовать библиотеку Azure Data Lake Analytics — ADAL для .Net, которая позволяет создавать и управлять заданиями. Второй вариант: использовать сервис Azure Data Factory, который можно сконфигурировать на периодический запуск сценариев USQL. Использовать синтаксис, близкий к SQL, очень удобно для разработки, отладки и обновления, что ускоряет процесс освоения этой системы. Как уже отмечалось, существенным отличием концепции Data Lake от DWH является то, что она поддерживает поток ELT. С одной стороны, это упрощает процедуру копирования, а с другой — несколько усложняет отладку, поскольку результат будет помещаться в файл. Таким образом, для визуализации информации, которая была извлечена с помощью сервиса Azure Data Lake Analytics, требуется сторонний специализированный сервис BI; Azure Data Lake Analytics в этом случае представляет собой движок преобразования файлов.\nКак уже было сказано выше, помимо Azure Data Lake Store, в качестве хранилища данных можно применить Azure BLOB Storage, что позволяет максимально гибко использовать сервис Azure Data Lake Analytics. Однако наибольшая производительность, гибкость и возможность интеграции с сервисами HDInsight для обеспечения наивысшей производительности достигается в случае применения Azure Data Lake Store.\n"
  ]
}